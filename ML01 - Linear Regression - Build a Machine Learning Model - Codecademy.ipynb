{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d2d25b",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "Machine learning can be branched out into the following categories:\n",
    "\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "Supervised Learning is where the data is labeled and the program learns to predict the output from the input data. For instance, a supervised learning algorithm for credit card fraud detection would take as input a set of recorded transactions. For each transaction, the program would predict if it is fraudulent or not.\n",
    "\n",
    "### Supervised learning \n",
    "problems can be further grouped into regression and classification problems.\n",
    "\n",
    "#### Regression:\n",
    "In regression problems, we are trying to predict a continuous-valued output. Examples are:\n",
    "- What is the housing price in Neo York?\n",
    "- What is the value of cryptocurrencies?\n",
    "\n",
    "#### Classification:\n",
    "In classification problems, we are trying to predict a discrete number of values. Examples are:\n",
    "- Is this a picture of a human or a picture of an AI?\n",
    "- Is this email spam?\n",
    "\n",
    "### Unsupervised Learning \n",
    "is a type of machine learning where the program learns the inherent structure of the data based on unlabeled examples.\n",
    "\n",
    "Clustering is a common unsupervised machine learning approach that finds patterns and structures in unlabeled data by grouping them into clusters.\n",
    "\n",
    "Some examples:\n",
    "- Social networks clustering topics in their news feed\n",
    "- Consumer sites clustering users for recommendations\n",
    "- Search engines to group similar objects in one cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef0cf4",
   "metadata": {},
   "source": [
    "### Scikit-learn \n",
    "is a library in Python that provides many unsupervised and supervised learning algorithms. It’s built upon some of the technology you might already be familiar with, like NumPy, pandas, and Matplotlib!\n",
    "\n",
    "The functionality that scikit-learn provides include:\n",
    "- Regression, including Linear and Logistic Regression\n",
    "- Classification, including K-Nearest Neighbors\n",
    "- Clustering, including K-Means and K-Means++\n",
    "- Model selection\n",
    "- Preprocessing, including Min-Max Normalization\n",
    "\n",
    "### Scikit-Learn Cheatsheet\n",
    "#### Linear Regression\n",
    "Import and create the model:\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "your_model = LinearRegression()\n",
    "```\n",
    "Fit:\n",
    "\n",
    "`your_model.fit(x_training_data, y_training_data)`\n",
    "\n",
    "`.coef_`: contains the coefficients\n",
    "`.intercept_`: contains the intercept\n",
    "\n",
    "Predict:\n",
    "\n",
    "`predictions = your_model.predict(your_x_data)`\n",
    "\n",
    "`.score()`: returns the coefficient of determination R²\n",
    "\n",
    "#### Naive Bayes\n",
    "Import and create the model:\n",
    "```\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "your_model = MultinomialNB()\n",
    "```\n",
    "Fit:\n",
    "\n",
    "`your_model.fit(x_training_data, y_training_data)`\n",
    "\n",
    "Predict:\n",
    "```\n",
    "# Returns a list of predicted classes - one prediction for every data point\n",
    "predictions = your_model.predict(your_x_data)\n",
    "\n",
    "# For every data point, returns a list of probabilities of each class\n",
    "probabilities = your_model.predict_proba(your_x_data)\n",
    "```\n",
    "\n",
    "#### K-Nearest Neighbors\n",
    "Import and create the model:\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "your_model = KNeighborsClassifier()\n",
    "```\n",
    "Fit:\n",
    "\n",
    "`your_model.fit(x_training_data, y_training_data)`\n",
    "\n",
    "Predict:\n",
    "```\n",
    "# Returns a list of predicted classes - one prediction for every data point\n",
    "predictions = your_model.predict(your_x_data)\n",
    "\n",
    "# For every data point, returns a list of probabilities of each class\n",
    "probabilities = your_model.predict_proba(your_x_data)\n",
    "```\n",
    "\n",
    "#### K-Means\n",
    "Import and create the model:\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "your_model = KMeans(n_clusters=4, init='random')\n",
    "```\n",
    "`n_clusters`: number of clusters to form and number of centroids to generate\n",
    "`init:` method for initialization\n",
    "- k-means++: K-Means++ [default]\n",
    "- random: K-Means\n",
    "`random_state`: the seed used by the random number generator [optional]\n",
    "\n",
    "Fit:\n",
    "\n",
    "`your_model.fit(x_training_data)`\n",
    "\n",
    "Predict:\n",
    "\n",
    "`predictions = your_model.predict(your_x_data)`\n",
    "\n",
    "#### Validating the Model\n",
    "Import and print accuracy, recall, precision, and F1 score:\n",
    "```\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(accuracy_score(true_labels, guesses))\n",
    "print(recall_score(true_labels, guesses))\n",
    "print(precision_score(true_labels, guesses))\n",
    "print(f1_score(true_labels, guesses))\n",
    "```\n",
    "Import and print the confusion matrix:\n",
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(true_labels, guesses))\n",
    "```\n",
    "Training Sets and Test Sets\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)\n",
    "\n",
    "train_size: the proportion of the dataset to include in the train split\n",
    "test_size: the proportion of the dataset to include in the test split\n",
    "random_state: the seed used by the random number generator [optional]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e6a6e",
   "metadata": {},
   "source": [
    "### Distance between 2 points\n",
    "points are usually represented in lists\n",
    "distance can be measured only between 2 points with the same dimensions\n",
    "\n",
    "#### 1. Euclidean Distance\n",
    "To find the Euclidean distance between two points, we first calculate the squared distance between each dimension. If we add up all of these squared differences and take the square root, we’ve computed the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbedbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(0,len(pt1)):\n",
    "    distance += (pt1[i] - pt2[i])**2\n",
    "  distance = distance**0.5\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4fdf0",
   "metadata": {},
   "source": [
    "#### 2. Manhattan Distance\n",
    "Sum the absolute value of the difference between each dimension. It’s called Manhattan distance because it’s similar to how you might navigate when walking city blocks. If you’ve ever wondered “how many blocks will it take me to get from point A to point B”, you’ve computed the Manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    distance += abs(pt1[i]-pt2[i])\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331453c",
   "metadata": {},
   "source": [
    "#### Hamming Distance\n",
    "Hamming Distance is another slightly different variation on the distance formula. Instead of finding the difference of each dimension, Hamming distance only cares about whether the dimensions are exactly equal. When finding the Hamming distance between two points, add one for every dimension that has different values.\n",
    "\n",
    "Hamming distance is used in spell checking algorithms. For example, the Hamming distance between the word “there” and the typo “thete” is one. Each letter is a dimension, and each dimension has the same value except for one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(pt1, pt2):\n",
    "  distance = 0\n",
    "  for i in range(len(pt1)):\n",
    "    if pt1[i] != pt2[i]:\n",
    "      distance += 1\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3290b9f",
   "metadata": {},
   "source": [
    "### SciPy Distances\n",
    "Now that you’ve written these three distance formulas yourself, let’s look at how to use them using Python’s SciPy library:\n",
    "\n",
    "- Euclidean Distance .euclidean()\n",
    "- Manhattan Distance .cityblock()\n",
    "- Hamming Distance .hamming()\n",
    "\n",
    "There are a few noteworthy details to talk about:\n",
    "- First, the scipy implementation of Manhattan distance is called cityblock(). Remember, computing Manhattan distance is like asking how many blocks away you are from a point.\n",
    "- Second, the scipy implementation of Hamming distance will always return a number between 0 an 1. Rather than summing the number of differences in dimensions, this implementation sums those differences and then divides by the total number of dimensions. For example, in your implementation, the Hamming distance between [1, 2, 3] and [7, 2, -10] would be 2. In scipy‘s version, it would be 2/3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c20a3",
   "metadata": {},
   "source": [
    "## Regression vs Classification\n",
    "\n",
    "**Regression** is used to predict outputs that are continuous. The outputs are quantities that can be flexibly determined based on the inputs of the model rather than being confined to a set of possible labels.\n",
    "\n",
    "For example:\n",
    "- Predict the height of a potted plant from the amount of rainfall\n",
    "- Predict salary based on someone’s age and availability of high-speed internet\n",
    "- Predict a car’s MPG (miles per gallon) based on size and model year\n",
    "\n",
    "Linear regression is the most popular regression algorithm. It is often underrated because of its relative simplicity. In a business setting, it could be used to predict the likelihood that a customer will churn or the revenue a customer will generate. More complex models may fit this data better, at the cost of losing simplicity.\n",
    "\n",
    "**Classification** is used to predict a discrete label. The outputs fall under a finite set of possible outcomes. Many situations have only two possible outcomes. This is called binary classification (True/False, 0 or 1, Hotdog / not Hotdog).\n",
    "\n",
    "For example:\n",
    "- Predict whether an email is spam or not\n",
    "- Predict whether it will rain or not\n",
    "- Predict whether a user is a power user or a casual user\n",
    "\n",
    "There are also two other common types of classification: **multi-class classification** and **multi-label** classification.\n",
    "Multi-class classification has the same idea behind binary classification, except instead of two possible outcomes, there are three or more.\n",
    "\n",
    "For example:\n",
    "- Predict whether a photo contains a pear, apple, or peach\n",
    "- Predict what letter of the alphabet a handwritten character is\n",
    "- Predict whether a piece of fruit is small, medium, or large\n",
    "\n",
    "in multi-label classification, there are multiple possible labels for each outcome. This is useful for customer segmentation, image categorization, and sentiment analysis for understanding text. To perform these classifications, we use models like Naive Bayes, K-Nearest Neighbors, SVMs, as well as various deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853a736",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "The purpose of machine learning is often to create a model that explains some real-world data, so that we can predict what may happen next, with different inputs.\n",
    "\n",
    "The simplest model that we can fit to data is a line. When we are trying to find a line that fits a set of data best, we are performing Linear Regression.\n",
    "\n",
    "We often want to find lines to fit data, so that we can predict unknowns. For example:\n",
    "- The market price of a house vs. the square footage of a house. Can we predict how much a house will sell for, given its size?\n",
    "- The tax rate of a country vs. its GDP. Can we predict taxation based on a country’s GDP?\n",
    "- The amount of chips left in the bag vs. number of chips taken. Can we predict how much longer this bag of chips will last, given how much people at this party have been eating?\n",
    "\n",
    "#### Points and Lines\n",
    "In the last exercise, you were probably able to make a rough estimate about the next data point for Sandra’s lemonade stand without thinking too hard about it. For our program to make the same level of guess, we have to determine what a line would look like through those data points.\n",
    "\n",
    "A line is determined by its slope and its intercept. In other words, for each point y on a line we can say:\n",
    "        **y = mx + b**\n",
    "\n",
    "where m is the slope, and b is the intercept. y is a given point on the y-axis, and it corresponds to a given x on the x-axis.\n",
    "\n",
    "The slope is a measure of how steep the line is, while the intercept is a measure of where the line hits the y-axis.\n",
    "\n",
    "When we perform Linear Regression, the goal is to get the “best” m and b for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8108bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change m and b to the best fit by trial and error:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]\n",
    "\n",
    "#slope:\n",
    "m = 5\n",
    "#intercept:\n",
    "b = 30\n",
    "y = [(m * x ) + b for x in months]\n",
    "\n",
    "plt.plot(months, revenue, \"x\")\n",
    "plt.plot(months, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d1663",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "When we think about how we can assign a slope and intercept to fit a set of points, we have to define what the best fit is.\n",
    "\n",
    "For each data point, we calculate loss, a number that measures how bad the model’s (in this case, the line’s) prediction was. You may have seen this being referred to as error.\n",
    "\n",
    "We can think about loss as the squared distance from the point to the line. We do the squared distance (instead of just the distance) so that points above and below the line both contribute to total loss in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have three points, (1, 5), (2, 1), and (3, 3). We are trying to find a line that produces lowest loss.\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = [5, 1, 3]\n",
    "\n",
    "#y = x\n",
    "m1 = 1\n",
    "b1 = 0\n",
    "y_predicted1 = [(m1*x) + b1 for x in x]\n",
    "\n",
    "#y = 0.5x + 1\n",
    "m2 = 0.5\n",
    "b2 = 1\n",
    "y_predicted2 = [(m2*x) + b2 for x in x]\n",
    "\n",
    "total_loss1 = 0\n",
    "for i in range(len(y)):\n",
    "  total_loss1 += (y[i] - y_predicted1[i])**2\n",
    "\n",
    "total_loss2 = 0\n",
    "for i in range(len(y)):\n",
    "  total_loss2 += (y[i] - y_predicted2[i])**2\n",
    "\n",
    "print(total_loss1)\n",
    "print(total_loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b447e",
   "metadata": {},
   "source": [
    "#### Gradient Descent for `Intercept`\n",
    "As we try to minimize loss, we take each parameter we are changing, and move it as long as we are decreasing loss. \n",
    "The process by which we do this is called gradient descent. We move in the direction that decreases our loss the most.\n",
    "\n",
    "**The calculation for this:**\n",
    "Basically:\n",
    "1. we find the sum of y_value - (m*x_value + b) for all the y_values and x_values we have\n",
    "2. and then we multiply the sum by a factor of -2/N. N is the number of points we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2946e95",
   "metadata": {},
   "source": [
    "#### Gradient Descent for `Slope`\n",
    "\n",
    "To find the m gradient:\n",
    "- we find the sum of x_value * (y_value - (m*x_value + b)) for all the y_values and x_values we have\n",
    "- and then we multiply the sum by a factor of -2/N. N is the number of points we have.\n",
    "\n",
    "Once we have a way to calculate both the m gradient and the b gradient, we’ll be able to follow both of those gradients downwards to the point of lowest loss for both the m value and the b value. Then, we’ll have the best m and the best b to fit our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c824662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_at_b(x, y, m, b):\n",
    "    diff = 0\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "      y_val = y[i]\n",
    "      x_val = x[i]\n",
    "      diff += (y_val - ((m * x_val) + b))\n",
    "    b_gradient = -2/N * diff\n",
    "    return b_gradient\n",
    "\n",
    "def get_gradient_at_m(x, y, m, b):\n",
    "  diff = 0\n",
    "  N = len(x)\n",
    "  for i in range(N):\n",
    "    y_val = y[i]\n",
    "    x_val = x[i]\n",
    "    diff += x_val * (y_val - (m * x_val + b))\n",
    "    m_gradient = -2/N * diff\n",
    "  return m_gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a26e13",
   "metadata": {},
   "source": [
    "Now that we know how to calculate the gradient, we want to take a “step” in that direction. However, it’s important to think about whether that step is too big or too small. We don’t want to overshoot the minimum error!\n",
    "\n",
    "We can scale the size of the step by multiplying the gradient by a learning rate.\n",
    "To find a new b value, we would say:\n",
    "\n",
    "`new_b = current_b - (learning_rate * b_gradient)`\n",
    "\n",
    "where current_b is our guess for what the b value is, b_gradient is the gradient of the loss curve at our current guess, and learning_rate is proportional to the size of the step we want to take.\n",
    "\n",
    "In a few exercises, we’ll talk about the implications of a large or small learning rate, but for now, let’s use a fairly small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7805611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to move the parameter values in the direction of the gradient at a rate of 0.01\n",
    "# Define your step_gradient function here\n",
    "def step_gradient(x, y, b_current, m_current):\n",
    "  b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n",
    "  m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n",
    "  b = b_current - (0.01 * b_gradient)\n",
    "  m = m_current - (0.01 * m_gradient)\n",
    "  return b, m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b170b",
   "metadata": {},
   "source": [
    "#### Convergence\n",
    "How do we know when we should stop changing the parameters m and b? How will we know when our program has learned enough?\n",
    "\n",
    "To answer this, we have to define convergence. Convergence is when the loss stops changing (or changes very slowly) when parameters are changed.\n",
    "\n",
    "#### Learning Rate\n",
    "We want our program to be able to iteratively learn what the best m and b values are. So for each m and b pair that we guess, we want to move them in the direction of the gradients we’ve calculated. But how far do we move in that direction?\n",
    "\n",
    "We have to choose a learning rate, which will determine how far down the loss curve we go.\n",
    "\n",
    "A small learning rate will take a long time to converge — you might run out of time or cycles before getting an answer. A large learning rate might skip over the best value. It might never converge! Oh no!\n",
    "\n",
    "Finding the absolute best learning rate is not necessary for training a model. You just have to find a learning rate large enough that gradient descent converges with the efficiency you need, and not so large that convergence never happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53755ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_gradient_at_b(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "    x_val = x[i]\n",
    "    y_val = y[i]\n",
    "    diff += (y_val - ((m * x_val) + b))\n",
    "  b_gradient = -(2/N) * diff  \n",
    "  return b_gradient\n",
    "\n",
    "def get_gradient_at_m(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "      x_val = x[i]\n",
    "      y_val = y[i]\n",
    "      diff += x_val * (y_val - ((m * x_val) + b))\n",
    "  m_gradient = -(2/N) * diff  \n",
    "  return m_gradient\n",
    "\n",
    "#Your step_gradient function here\n",
    "def step_gradient(b_current, m_current, x, y, learning_rate):\n",
    "    b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n",
    "    m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n",
    "    b = b_current - (learning_rate * b_gradient)\n",
    "    m = m_current - (learning_rate * m_gradient)\n",
    "    return [b, m]\n",
    "  \n",
    "#Your gradient_descent function here:  \n",
    "def gradient_descent(x,y,learning_rate, num_iterations):\n",
    "  b = 0\n",
    "  m = 0\n",
    "  for i in range(num_iterations):\n",
    "    b, m = step_gradient(b,m,x,y,learning_rate)\n",
    "  return b, m\n",
    "\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]\n",
    "\n",
    "b, m = gradient_descent(months, revenue, 0.01, 1000)\n",
    "\n",
    "y = [m*x + b for x in months]\n",
    "\n",
    "plt.plot(months, revenue, \"o\")\n",
    "plt.plot(months, y)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37a530",
   "metadata": {},
   "source": [
    "#### Scikit-Learn\n",
    "Congratulations! You’ve now built a linear regression algorithm from scratch.\n",
    "\n",
    "Luckily, we don’t have to do this every time we want to use linear regression. We can use Python’s scikit-learn library. Scikit-learn, or sklearn, is used specifically for Machine Learning. Inside the linear_model module, there is a LinearRegression() function we can use:\n",
    "\n",
    "`from sklearn.linear_model import LinearRegression`\n",
    "\n",
    "You can first create a LinearRegression model, and then fit it to your x and y data:\n",
    "```\n",
    "line_fitter = LinearRegression()\n",
    "line_fitter.fit(X, y)\n",
    "```\n",
    "The .fit() method gives the model two variables that are useful to us:\n",
    "\n",
    "the `line_fitter.coef_`, which contains the slope\n",
    "the `line_fitter.intercept_`, which contains the intercept\n",
    "We can also use the .predict() function to pass in x-values and receive the y-values that this line would predict:\n",
    "\n",
    "`y_predicted = line_fitter.predict(X)`\n",
    "\n",
    "Note: the num_iterations and the learning_rate that you learned about in your own implementation have default values within scikit-learn, so you don’t need to worry about setting them specifically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3_seaborn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "temperature = np.array(range(60, 100, 2))\n",
    "temperature = temperature.reshape(-1, 1)\n",
    "sales = [65, 58, 46, 45, 44, 42, 40, 40, 36, 38, 38, 28, 30, 22, 27, 25, 25, 20, 15, 5]\n",
    "\n",
    "# Create an sklearn linear regression model and call it line_fitter\n",
    "line_fitter = LinearRegression()\n",
    "\n",
    "# Fit the line_fitter object to temperature and sales.\n",
    "line_fitter.fit(temperature, sales)\n",
    "\n",
    "# Create a list called sales_predict that is the predicted sales values that line_fitter would generate from the temperature list\n",
    "sales_predict = line_fitter.predict(temperature)\n",
    "\n",
    "plt.plot(temperature, sales, 'o')\n",
    "plt.plot(temperature, sales_predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a784f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50144fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7edec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d1ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222f7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a189c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59cfc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28af9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
