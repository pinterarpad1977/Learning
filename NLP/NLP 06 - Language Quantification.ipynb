{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cad1fd3",
   "metadata": {},
   "source": [
    "### Bag-of-What?\n",
    "Bag-of-words (BoW) is a statistical language model based on word count. Say what?\n",
    "\n",
    "Let’s start with that first part: a **statistical language model** is a way for computers to make sense of language based on probability. For example, let’s say we have the text:\n",
    "\n",
    "“Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?”\n",
    "\n",
    "A statistical language model focused on the starting letter for words might take this text and predict that words are most likely to start with the letter “f” because 11 out of 15 words begin that way. A different statistical model that pays attention to word order might tell us that the word “fish” tends to follow the word “fantastic.”\n",
    "\n",
    "Bag-of-words does not give a flying fish about word starts or word order though; its sole concern is word count — how many times each word appears in a document.\n",
    "\n",
    "If you’re already familiar with statistical language models, you may also have heard BoW referred to as the **unigram model**. It’s technically **a special case of another statistical model, the n-gram model**, with n (the number of words in a sequence) set to 1.\n",
    "\n",
    "The words from the sentence go into the bag-of-words and come out as a dictionary of words with their corresponding counts. For statistical models, we call the text that we use to build the model our **training data**. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ee47ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d45e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 2, 'love': 1, 'fantastic': 2, 'fly': 2, 'fish': 3, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1}\n"
     ]
    }
   ],
   "source": [
    "# preprocessing the text:\n",
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "  return normalized\n",
    "\n",
    "# Define text_to_bow() below:\n",
    "def text_to_bow(some_text):\n",
    "  bow_dictionary = {}\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    if token in bow_dictionary.keys():\n",
    "      bow_dictionary[token] +=1\n",
    "    else:\n",
    "      bow_dictionary[token] = 1\n",
    "  return bow_dictionary\n",
    "print(text_to_bow(\"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38854fc9",
   "metadata": {},
   "source": [
    "### Building a Features Dictionary\n",
    "\n",
    "1. clean the training data, \n",
    "2. tokenize it\n",
    "3. normalize (lemmatize with pos) it\n",
    "4. create a dictionary with the unique words as keys and assign indexes to them  (instead of counting them as above)\n",
    "\n",
    "This dictionary will function as a **vocabulary** in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_dictionary(documents):\n",
    "  features_dictionary = {}\n",
    "  merged = ' '.join(documents)\n",
    "  tokens = preprocess_text(merged)\n",
    "  index = 0\n",
    "  for token in tokens:\n",
    "    if token not in features_dictionary.keys():\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "  return features_dictionary, tokens\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "\n",
    "print(create_features_dictionary(training_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6dc05",
   "metadata": {},
   "source": [
    "### Building a BoW Vector\n",
    "\n",
    "We can represent a vector as a list.\n",
    "The indexes will represent a word and the value will be set to its count\n",
    "If I understand correctly the vector will show which words and how many times appear from our vocabulary appears in some test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9143651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow_vector(some_text, features_dictionary):\n",
    "  bow_vector = [0 for x in range(len(features_dictionary)) ]\n",
    "  tokens = preprocess_text(some_text)\n",
    "  for token in tokens:\n",
    "    if token in features_dictionary.keys():\n",
    "      feature_index = features_dictionary[token]\n",
    "      bow_vector[feature_index] += 1\n",
    "  return bow_vector, tokens\n",
    "\n",
    "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(text_to_bow_vector(text, features_dictionary)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61050177",
   "metadata": {},
   "source": [
    "It seems that once we have the features dictionary and the BOW vector we can train the model on test text (eg spam emails)\n",
    "Then we can create test vectors to test the accuracy of the model.\n",
    "However for this they used something called Naive Bayes classification which was not explained so for now I won't add the code below.\n",
    "\n",
    "Yup as they say in the next lesson, all of this was not necessary, because there are libraries and functions already doing it:\n",
    "\n",
    "### Spam A Lot No More\n",
    "Amazing work! As is the case with many tasks in Python, there’s already a library that can do all of that work for you.\n",
    "\n",
    "For `text_to_bow()`, you can approximate the functionality with the collections module’s Counter() function:\n",
    "```\n",
    "from collections import Counter\n",
    "\n",
    "tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']\n",
    "print(Counter(tokens))\n",
    "\n",
    "# Counter({'fish': 2, 'another': 2, 'find': 1, 'five': 1, 'faraway': 1})\n",
    "```\n",
    "For vectorization, you can use CountVectorizer from the machine learning library scikit-learn. You can use fit() to train the features dictionary and then transform() to transform text into a vector:\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "test_text = [\"Another five fish find another faraway fish.\"]\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_vectorizer.fit(training_documents)\n",
    "bow_vector = bow_vectorizer.transform(test_text)\n",
    "print(bow_vector.toarray())\n",
    "# [[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a6b0d",
   "metadata": {},
   "source": [
    "Hopefully we'll get there soon that I understand the whole code, however I learnt this from Bing:\n",
    "I was wondering why different method was used for the training and the test data in the vectorization exercise:\n",
    "```\n",
    "# for training data:\n",
    "vector_from_training_data = vectorizer.fit_transform(training_data)\n",
    "\n",
    "# for test data:\n",
    "vector_from_test_data = vectorizer.transform(test_data)\n",
    "```\n",
    "\n",
    "So Bing says thatthis prevents data leakage from the training data to the test data.\n",
    "- For the training vectors the `.fit()` or the `.fit_transform()` method is used\n",
    "- For the test data you need only to `.transform()`\n",
    "\n",
    "the only thing remained to understand from this poart is how this MultinomialNB works:\n",
    "`from sklearn.naive_bayes import MultinomialNB`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f7a95",
   "metadata": {},
   "source": [
    "### BoW Wow\n",
    "As you can see, bag-of-words is pretty useful! BoW also has several advantages over other language models. For one, it’s an easier model to get started with and a few Python libraries already have built-in support for it.\n",
    "\n",
    "Because bag-of-words relies on single words, rather than sequences of words, there are more examples of each unit of language in the training corpus. More examples means the model has less **data sparsity** (i.e., it has more training knowledge to draw from) than other statistical models.\n",
    "\n",
    "Imagine you want to make a shirt to sell to people. If you have the shirt exactly tailored to someone’s body, it probably won’t fit that many people. But if you make a shirt that is just a giant bag with arm holes, you know that no one will buy it. What do you do? You loosely fit the shirt to someone’s body, leaving some extra room for different body shapes.\n",
    "\n",
    "**Overfitting** (adapting a model too strongly to training data, akin to our highly tailored shirt) is a common problem for statistical language models. While BoW still suffers from overfitting in terms of vocabulary, it overfits less than other statistical models, allowing for more flexibility in grammar and word choice.\n",
    "\n",
    "The combination of low data sparsity and less overfitting makes the bag-of-words model more reliable with smaller training data sets than other statistical models.\n",
    "\n",
    "\n",
    "The snippet below demonstrates how much more effective BoW is compared to a bigram if the training data is scarce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961e36ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three most frequent word sequences and the number of occurrences according to Bigrams:\n",
      "[(('it', 's'), 1), (('s', 'excite'), 1), (('excite', 'to'), 1)]\n",
      "\n",
      "Three most frequent words and number of occurrences according to Bag-of-Words:\n",
      "[('fish', 4), ('fly', 3), ('day', 3)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"It's exciting to watch flying fish after a hard day's work. I don't know why some fish prefer flying and other fish would rather swim. It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'\"\n",
    "tokens = preprocess_text(text)\n",
    "\n",
    "# Bigram approach:\n",
    "bigrams_prepped = ngrams(tokens, 2)\n",
    "bigrams = Counter(bigrams_prepped)\n",
    "print(\"Three most frequent word sequences and the number of occurrences according to Bigrams:\")\n",
    "print(bigrams.most_common(3))\n",
    "\n",
    "# Bag-of-Words approach:\n",
    "# Define bag_of_words here:\n",
    "bag_of_words = Counter(tokens)\n",
    "print(\"\\nThree most frequent words and number of occurrences according to Bag-of-Words:\")\n",
    "most_common_three = bag_of_words.most_common(3)\n",
    "print(most_common_three)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e68c3",
   "metadata": {},
   "source": [
    "### BoW Ow\n",
    "Alas, there is a trade-off for all the brilliance BoW brings to the table.\n",
    "\n",
    "Unless you want sentences that look like “the a but for the”, BoW is NOT a great primary model for text prediction. If that sort of “sentence” isn’t your bag, it’s because **bag-of-words has high perplexity**, meaning that it’s not a very accurate model for language prediction. The probability of the following word is always just the most frequently used words.\n",
    "\n",
    "If your BoW model finds “good” frequently occurring in a text sample, you might assume there’s a positive sentiment being communicated in that text… but if you look at the original text you may find that in fact every “good” was preceded by a “not.”\n",
    "\n",
    "Hmm, that would have been helpful to know. The BoW model’s word tokens lack context, which can make a word’s intended meaning unclear.\n",
    "\n",
    "Perhaps you are wondering, “What happens if the model comes across a new word that wasn’t in the training data?” As mentioned, like all statistical models, BoW suffers from overfitting when it comes to vocabulary.\n",
    "\n",
    "There are several ways that NLP developers have tackled this issue. A common approach is through **language smoothing** in which some probability is siphoned from the known words and given to unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4492e2",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75688e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
