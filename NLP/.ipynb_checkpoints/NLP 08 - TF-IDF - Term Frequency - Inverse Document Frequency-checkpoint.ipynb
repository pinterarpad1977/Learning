{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d813aed6",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency\n",
    "\n",
    "Is a language model commonly known as tf-idf. Tf-idf is another powerful tool in your NLP toolkit that has a variety of use cases included:\n",
    "- ranking results in a search engine\n",
    "- text summarization\n",
    "- building smarter chatbots\n",
    "\n",
    "The output of applying tf-idf is a table, also known as a term-document matrix. You can think of a term-document matrix like a matrix of bag-of-word vectors.\n",
    "Each column of the table represents a unique document (in this case, an individual sentence). Each row represents a unique word token. The value in each cell represents the tf-idf score for a word token in that particular document.\n",
    "\n",
    "Term frequency-inverse document frequency is a numerical statistic used to indicate how important a word is to each document in a collection of documents, or a corpus.\n",
    "\n",
    "When applying tf-idf to a corpus, each word is given a tf-idf score for each document, representing the relevance of that word to the particular document. A higher tf-idf score indicates a term is more important to the corresponding document.\n",
    "\n",
    "Tf-idf has many similarities with the bag-of-words language model, which if you recall is concerned with word count — how many times each word appears in a document.\n",
    "\n",
    "While tf-idf can be used in any situation bag-of-words can be used, there is a key difference in how it is calculated.\n",
    "\n",
    "Tf-idf relies on two different metrics in order to come up with an overall score:\n",
    "- term frequency, or how often a word appears in a document. This is the same as bag-of-words’ word count.\n",
    "- inverse document frequency, which is a measure of how often a word appears in the overall corpus. By penalizing the score of words that appear throughout a corpus, tf-idf can give better insight into how important a word is to a particular document of a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d0d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized])\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb10903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sample documents\n",
    "document_1 = \"This is a sample sentence!\"\n",
    "document_2 = \"This is my second sentence.\"\n",
    "document_3 = \"Is this my third sentence?\"\n",
    "\n",
    "# corpus of documents\n",
    "corpus = [document_1, document_2, document_3]\n",
    "\n",
    "# preprocess documents\n",
    "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tf_idf_scores = vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "# get vocabulary of terms\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "corpus_index = [n for n in processed_corpus]\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
    "print(df_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1b4fe",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "The inverse document frequency component of the tf-idf score penalizes terms that appear more frequently across a corpus. The intuition is that words that appear more frequently in the corpus give less insight into the topic or meaning of an individual document, and should thus be deprioritized.\n",
    "\n",
    "For example, terms like “the” or “go” are used all over the place, so in a bag-of-words model, they would be given priority even though they don’t provide much meaning; tf-idf would deprioritize these sorts of common words.\n",
    "\n",
    "Inverse document frequency can be calculated on a group of documents using scikit-learn’s TfidfTransformer:\n",
    "```\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "transformer.fit(term_frequencies)\n",
    "inverse_doc_frequency = transformer.idf_\n",
    "```\n",
    "- a TfidfTransformer object is initialized. Don’t worry about the norm=None keyword argument for now, we will dig into this in the next exercise\n",
    "- the TfidfTransformer is fit (trained) on a term-document matrix of term frequencies\n",
    "- the .idf_ attribute of the TfidfTransformer stores the inverse document frequencies of the terms as a NumPy array\n",
    "\n",
    "\n",
    "We can easily calculate the tf-idf values for each term-document pair in our corpus using scikit-learn’s TfidfVectorizer:\n",
    "```\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "tfidf_vectorizer = vectorizer.fit_transform(corpus)\n",
    "```\n",
    "- a TfidfVectorizer object is initialized. The norm=None keyword argument prevents scikit-learn from modifying the multiplication of term frequency and inverse document frequency\n",
    "- the TfidfVectorizer object is fit and transformed on the corpus of data, returning the tf-idf scores for each term-document pair\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b34554",
   "metadata": {},
   "source": [
    "### Converting Bag-of-Words to Tf-idf\n",
    "In addition to directly calculating the tf-idf scores for a set of terms across a corpus, you can also convert a bag-of-words model you have already created into tf-idf scores.\n",
    "\n",
    "Scikit-learn’s TfidfTransformer is up to the task of converting your bag-of-words model to tf-idf. You begin by initializing a TfidfTransformer object.\n",
    "\n",
    "`tf_idf_transformer = TfidfTransformer(norm=False)`\n",
    "\n",
    "Given a bag-of-words matrix `count_matrix`, you can now multiply the term frequencies by their inverse document frequency to get the tf-idf scores as follows:\n",
    "\n",
    "`tf_idf_scores = tfidf_transformer.fit_transform(count_matrix)`\n",
    "\n",
    "This is very similar to how we calculated inverse document frequency, except this time we are fitting and transforming the TfidfTransformer to the term frequencies/bag-of-words vectors rather than just fitting the TfidfTransformer to them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0a7bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import import_ipynb\n",
    "from the_raven import the_raven_stanzas\n",
    "\n",
    "# view first stanza\n",
    "print(the_raven_stanzas[0])\n",
    "\n",
    "# preprocess documents\n",
    "processed_stanzas = [preprocess_text(stanza) for stanza in the_raven_stanzas]\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# get vocabulary of terms\n",
    "tfidf_scores = vectorizer.fit_transform(processed_stanzas)\n",
    "\n",
    "# get stanza index\n",
    "stanza_index = [f\"Stanza {i+1}\" for i in range(len(the_raven_stanzas))]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# create pandas DataFrame with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=stanza_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2f8dc",
   "metadata": {},
   "source": [
    "### Working with Text Data | scikit-learn | From Occurrences to Frequencies\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#from-occurrences-to-frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ef2b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karachi wholesale market rat for sugar drop to less than r 50 per kg follow the resumption of sugar cane crush by sugar mill in sindh within two day the rate drop by r 1 70 to r 49 80 per kg in karachi whole sale market accord to dealer the resumption of sugar cane crush by the mill stabilise the supply to the market with an immediate effect on price a well industry expert say that the quality of sugar cane be excellent in sindh and approximately 100 kg of sugar cane can produce 11 kg of sugar\n",
      "  Are the tf-idf scores the same?\n",
      "0                             YES\n",
      "       Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "100            0          0          1          0          0          0   \n",
      "11             0          0          1          0          0          0   \n",
      "15             0          0          0          0          1          0   \n",
      "158            0          1          0          0          0          0   \n",
      "19             0          1          0          0          0          0   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "world          0          0          0          0          0          3   \n",
      "would          0          0          0          1          0          0   \n",
      "year           0          1          0          0          0          0   \n",
      "yi             0          0          0          0          0          0   \n",
      "yuan           0          0          0          0          0          0   \n",
      "\n",
      "       Article 7  Article 8  Article 9  Article 10  \n",
      "100            0          0          0           0  \n",
      "11             0          0          0           0  \n",
      "15             0          0          0           0  \n",
      "158            0          0          0           0  \n",
      "19             0          0          0           0  \n",
      "...          ...        ...        ...         ...  \n",
      "world          0          0          0           0  \n",
      "would          0          0          1           0  \n",
      "year           0          0          0           0  \n",
      "yi             0          0          0           2  \n",
      "yuan           0          0          0           2  \n",
      "\n",
      "[374 rows x 10 columns]\n",
      "       Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "100          0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "11           0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "15           0.0   0.000000   0.000000   0.000000   2.704748   0.000000   \n",
      "158          0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "19           0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "world        0.0   0.000000   0.000000   0.000000   0.000000   8.114244   \n",
      "would        0.0   0.000000   0.000000   2.299283   0.000000   0.000000   \n",
      "year         0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "yi           0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "yuan         0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "\n",
      "       Article 7  Article 8  Article 9  Article 10  \n",
      "100          0.0        0.0   0.000000    0.000000  \n",
      "11           0.0        0.0   0.000000    0.000000  \n",
      "15           0.0        0.0   0.000000    0.000000  \n",
      "158          0.0        0.0   0.000000    0.000000  \n",
      "19           0.0        0.0   0.000000    0.000000  \n",
      "...          ...        ...        ...         ...  \n",
      "world        0.0        0.0   0.000000    0.000000  \n",
      "would        0.0        0.0   2.299283    0.000000  \n",
      "year         0.0        0.0   0.000000    0.000000  \n",
      "yi           0.0        0.0   0.000000    5.409496  \n",
      "yuan         0.0        0.0   0.000000    5.409496  \n",
      "\n",
      "[374 rows x 10 columns]\n",
      "       Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "100          0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "11           0.0   0.000000   2.704748   0.000000   0.000000   0.000000   \n",
      "15           0.0   0.000000   0.000000   0.000000   2.704748   0.000000   \n",
      "158          0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "19           0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "world        0.0   0.000000   0.000000   0.000000   0.000000   8.114244   \n",
      "would        0.0   0.000000   0.000000   2.299283   0.000000   0.000000   \n",
      "year         0.0   2.704748   0.000000   0.000000   0.000000   0.000000   \n",
      "yi           0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "yuan         0.0   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "\n",
      "       Article 7  Article 8  Article 9  Article 10  \n",
      "100          0.0        0.0   0.000000    0.000000  \n",
      "11           0.0        0.0   0.000000    0.000000  \n",
      "15           0.0        0.0   0.000000    0.000000  \n",
      "158          0.0        0.0   0.000000    0.000000  \n",
      "19           0.0        0.0   0.000000    0.000000  \n",
      "...          ...        ...        ...         ...  \n",
      "world        0.0        0.0   0.000000    0.000000  \n",
      "would        0.0        0.0   2.299283    0.000000  \n",
      "year         0.0        0.0   0.000000    0.000000  \n",
      "yi           0.0        0.0   0.000000    5.409496  \n",
      "yuan         0.0        0.0   0.000000    5.409496  \n",
      "\n",
      "[374 rows x 10 columns]\n",
      "Article 1    fare\n",
      "dtype: object\n",
      "Article 2    hong\n",
      "dtype: object\n",
      "Article 3    sugar\n",
      "dtype: object\n",
      "Article 4    petrol\n",
      "dtype: object\n",
      "Article 5    engine\n",
      "dtype: object\n",
      "Article 6    australia\n",
      "dtype: object\n",
      "Article 7    car\n",
      "dtype: object\n",
      "Article 8    railway\n",
      "dtype: object\n",
      "Article 9    cabinet\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocess_text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from articles import articles\n",
    "\n",
    "# import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# preprocess articles\n",
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "print(processed_articles[2])\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "# convert counts to tf-idf\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "tfidf_scores_transformed = transformer.fit_transform(counts)\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm= None)\n",
    "tfidf_scores = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "\n",
    "# check if tf-idf scores are equal\n",
    "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['YES']}))\n",
    "else:\n",
    "  print(pd.DataFrame({'Are the tf-idf scores the same?':['No, something is wrong :(']}))\n",
    "\n",
    "\n",
    "# get vocabulary of terms\n",
    "try:\n",
    "  feature_names = vectorizer.get_feature_names_out()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get article index\n",
    "try:\n",
    "  article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame with word counts\n",
    "try:\n",
    "  df_word_counts = pd.DataFrame(counts.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_word_counts)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# create pandas DataFrame(s) with tf-idf scores\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores_transformed.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
    "  print(df_tf_idf)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# get highest scoring tf-idf term for each article\n",
    "for i in range(1,10):\n",
    "  print(df_tf_idf[[f'Article {i}']].idxmax())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c4964",
   "metadata": {},
   "source": [
    "The Pandas Series method .idxmax() is a helpful tool for returning the index of the highest value in a DataFrame column. We will use this method to find the highest scoring tf-idf term for each article.\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmax.html\n",
    "\n",
    "Within the for loop, paste the following code:\n",
    "\n",
    "`print(df_tf_idf[[f'Article {i}']].idxmax())`\n",
    "\n",
    "On each pass through the for loop, this code will print the index of the term with the highest tf-idf score for that article (from Article 1 to Article 10)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
