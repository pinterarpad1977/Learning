{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f432078-4c51-4142-abe8-13dc62d4cb31",
   "metadata": {},
   "source": [
    "## Preprocessing for seq2seq\n",
    "\n",
    "This is now the second time I try to understand the deep learning model.\n",
    "I wanted to explain to myself now every step for future reference. Just following up the exercises left a lot of open questions.\n",
    "\n",
    "In this example we build a limited ENG-ESP translator.\n",
    "\n",
    "**Keras implementation** needs 3 things:\n",
    "1. vocabulary sets for input and target data (ENG + ESP)\n",
    "2. total number of unique word tokens for each set\n",
    "3. the maximum sentence length for each language\n",
    "\n",
    "Preprocessing:\n",
    "- noise removal (case, puntuation)\n",
    "- mark the start and the end of each sentence (like adding <START> + <END> to the text)\n",
    "\n",
    "Our data is in the span-eng.txt in this folder.\n",
    "Further remarks will be in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e2e9c4-6096-4ed2-b62f-9badb3c74c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import re\n",
    "# Importing our translations\n",
    "data_path = \"span-eng.txt\"\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "\n",
    "# Building empty lists to hold sentences\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "# Building empty vocabulary sets to hold the unique words from each language\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "for line in lines:\n",
    "  # Input and target sentences are separated by tabs\n",
    "  # we split them into two variables\n",
    "  input_doc, target_doc = line.split('\\t')\n",
    "  # Appending each input sentence to input_docs\n",
    "  input_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc))\n",
    "  input_docs.append(input_doc)\n",
    "  # Splitting words from punctuation\n",
    "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "  # Redefine target_doc below (adding the start and end marks)\n",
    "  # and append it to target_docs:\n",
    "  target_doc = \"<START> \" + target_doc + \" <END>\" \n",
    "  target_docs.append(target_doc)\n",
    "\n",
    "  \n",
    "  # Now we split up each sentence into words\n",
    "  # and add each unique word to our vocabulary set\n",
    "  for token in input_doc.split():\n",
    "    #print(token)\n",
    "    if token not in input_tokens:\n",
    "      input_tokens.add(token)\n",
    "    \n",
    "  for token in target_doc.split():\n",
    "    #print(token)\n",
    "    if token not in target_tokens:\n",
    "      target_tokens.add(token)\n",
    "    \n",
    "# it seems we cast the vocabulary sets to sorted lists. Why?\n",
    "# sets in Python are unordered, so we need to convert them to lists to make them ordered \n",
    "# this enables the item to item match between the two lists\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "# print(input_tokens, '\\n', target_tokens)\n",
    "# Create num_encoder_tokens and num_decoder_tokens:\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "# it is very much unclear what is the length of the sequence. This exercise counts the words, \n",
    "# but the keras tutorial simply counts the characters in the input and target docs\n",
    "# Chat-GPT says that if we work with words, then it must be the number of words else the number of chars\n",
    "'''for target_doc in target_docs:\n",
    "  print(target_doc, len(target_doc))\n",
    "for target_doc in target_docs:\n",
    "  target = re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)\n",
    "  print(target, len(target))'''\n",
    "\n",
    "# Still it is annoying that we split the <START> and <END> to 6 additional words and using the regex here again seems to be unnecessary\n",
    "try:\n",
    "  max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "  print(max_encoder_seq_length)\n",
    "  max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "  print(max_decoder_seq_length)\n",
    "except ValueError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6733b5d-0c4a-43a7-a4ee-322785637588",
   "metadata": {},
   "source": [
    "### The training setup\n",
    "\n",
    "Keras needs a matrix of one-hot vectors.\n",
    "In a one-hot vector every word is represented with a 0 in our sentence, except the one token we are currently working on.\n",
    "To follow up on these one-hot vectors we create 2 feature dictionaries and 2 reverse feature dictionaries.\n",
    "- one for the input data one-hot vectors (for the encoder)\n",
    "- one for the target data one-hot vectors ( for the encoder)\n",
    "- one for translating back the input one-hot vectors to actual words (for the decoder)\n",
    "- one for translating back the target one-hot vectors to actual words (for the decoder)\n",
    "\n",
    "To store the vectors we will create a 3D numpy array of zeroes in the size of\n",
    "1. number of input documents\n",
    "2. max length of the input sequence\n",
    "3. number of unique tokens in the vocabulary\n",
    "\n",
    "Actually we will need 3 of these arrays:\n",
    "1. encoder input data\n",
    "2. decoder input data\n",
    "3. decoder target data\n",
    "\n",
    "The decoder_target_data is needed for the \"Teacher forcing\" technique.\n",
    "It helps the encoder by having the target value ready, so the model doesn't have to rely on its own predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e8668-3987-41e4-aec6-920a929a55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# enumerate creates an iterable with the index of an item and the item itself\n",
    "# NOTE that it is swapped when added to the dicttionary, so the key is the item and its index is the value\n",
    "input_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(input_tokens)])\n",
    "target_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "# another swap of keys and values in the reverse dictionaries\n",
    "reverse_input_features_dict = dict(\n",
    "    (i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict(\n",
    "    (i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# enumerate(zip()) creates an iterable of sets with their indexes\n",
    "# line is the index and (iput_doc, target_doc) is the item (a set)\n",
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "\n",
    "  # another enumerate gives back the index of the words returned by the regex and the word\n",
    "  # this part in only for the input_docs which go to the encoder_input_data\n",
    "  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "\n",
    "    #print(\"Encoder input timestep & token:\", timestep, token)\n",
    "    #print(input_features_dict[token])\n",
    "    # Assign 1. for the current line, timestep, & word in encoder_input_data:\n",
    "    # this will create the one-hot vector for the token\n",
    "    encoder_input_data[line, timestep, input_features_dict[token]] = 1\n",
    "\n",
    "  # now the target_docs go to the decoder_input_data\n",
    "  for timestep, token in enumerate(target_doc.split()):\n",
    "\n",
    "    print(\"Decoder input timestep & token:\", timestep, token)\n",
    "    # Assign 1. for the current line, timestep, & word in decoder_input_data:\n",
    "    decoder_input_data[line, timestep, target_features_dict[token]] = 1\n",
    "    if timestep > 0:\n",
    "      # decoder_target_data is ahead by 1 timestep and doesn't include the start token.\n",
    "      print(\"Decoder target timestep:\", timestep, token)\n",
    "      # Assign 1. for the current line, timestep, & word in decoder_target_data:\n",
    "      decoder_target_data[line, timestep-1, target_features_dict[token]] = 1\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78b27e-9cd4-453b-86ea-ad2769b5b982",
   "metadata": {},
   "source": [
    "### Encoder Training Setup\n",
    "\n",
    "Keras deep learning models need 2 types of layers:\n",
    "1. input layer with the matrix of the one-hot vectors\n",
    "2. LSTM layer with outputs\n",
    "\n",
    "For the input layer we can specify the batch size, but also can select None, because it can handle varying batch sizes\n",
    "Additionally it needs the number of encoder tokens\n",
    "\n",
    "For the LSTM layer we need to specify the dimensionality (defines how closely the model molds itself to the training data)\n",
    "and we need to indicat whether we want to return the state.\n",
    "We want to have only the final state back from the encoder so we add the input layer as argument to it.\n",
    "The LSTM layer will retunr 3 things:\n",
    "1. encoder outputs  - we don't need it\n",
    "2. hidden state\n",
    "3. cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d864ff-31f7-4ee6-bcef-7ef0a462966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "\n",
    "# Create the input layer:\n",
    "encoder_inputs = Input( shape = (None, num_encoder_tokens))\n",
    "\n",
    "# Create the LSTM layer:\n",
    "encoder_lstm = LSTM(256, return_state = True)\n",
    "\n",
    "# Retrieve the outputs and states:\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Put the states together in a list:\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e026715-503d-405c-835d-b7b6b3e44f3c",
   "metadata": {},
   "source": [
    "### Decoder Training Setup\n",
    "\n",
    "The decoder has the same layers as the encoder.\n",
    "However we pass in the encoder states as the initial state along with the decoder inputs\n",
    "\n",
    "The LSTM layer will produce also 3 outputs:\n",
    "1. decoder outputs - we need it\n",
    "2. decoder hidden state - don't need\n",
    "3. decoder cell state - don't need\n",
    "\n",
    "The decoder output must be run through an activation layer which will return a probability distribution (sum to 1).\n",
    "We'll use the 'Softmax' function\n",
    "We'll use the Dense layer type for the decoder outputs to return the unique words in our vocabulary by using the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c6e77-6933-4851-a082-5074df81d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# The decoder input and LSTM layers:\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "\n",
    "# Retrieve the LSTM outputs and states:\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "\n",
    "# Build a final Dense layer:\n",
    "decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
    "\n",
    "# Filter outputs through the Dense layer:\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6226da-a7e4-4753-9eeb-eb3ca43afcd2",
   "metadata": {},
   "source": [
    "### Build and Train the model\n",
    "\n",
    "We need to **import the Model()** function from Keras.\n",
    "To define the model as seq2seq it needs 2 arguments:\n",
    "- encoder_inputs and decoder_inputs as a list\n",
    "- decoder_outputs\n",
    "\n",
    "Additinally we need to **compile the model** with 2 compulsory arguments:\n",
    "- it needs an optimizer (we use rmsprop) - this helps to minimize the error rate\n",
    "- it needs a loss function to determine the error rate (we use logarithm-based-cross-entropy function)\n",
    "- additionally we add accuracy as a metrics becasue we want to see this metric during the training\n",
    "\n",
    "Lastly we need to fit the compiled model, it needs the following arguments:\n",
    "- encoder input data and decoder input data as a list (what to train on)\n",
    "- decoder target data (what we expect to return)\n",
    "- batch size (indefinite: some problems require smaller batch sizes, other big)\n",
    "- epochs (cycles of training. more cycle mean the model will be more traind on the dataset and it will take more time)\n",
    "- validation_split (what percentage of the data is for validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47fd08-5b6f-4ede-852f-60c6bcb13bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Decoder training setup:\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Building the training model:\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "print(\"Model summary:\\n\")\n",
    "training_model.summary()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Compile the model:\n",
    "training_model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Choose the batch size\n",
    "# and number of epochs:\n",
    "batch_size = 5\n",
    "epochs = 100\n",
    "\n",
    "print(\"Training the model:\\n\")\n",
    "# Train the model:\n",
    "training_model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a0d52-100e-4096-b046-17b85ed5ca46",
   "metadata": {},
   "source": [
    "### Setup for Testing\n",
    "\n",
    "To actually generate text we need to modify the seq2seq architecture. \n",
    "In the current setup it works only because we know the target sequence.\n",
    "\n",
    "1. we need an encoder Model with placeholders for encoder inputs and encoder states\n",
    "   `encoder_model = Model(encoder_inputs, encoder_states)`\n",
    "2. we need placeholders for the decoder input states - these will be the input layer for the decoder\n",
    "   `\n",
    "4. we'll need to make everything step-by-step:\n",
    "       - pass the encoder's final hidden state to the decoder\n",
    "       - sample a token\n",
    "       - get the updated hidden state back\n",
    "       - pass the updated hidden state back into the net    ```\n",
    "    latent_dim = 256\n",
    "    decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "   ```\n",
    "3. We'll create new decoder states and outputs by using the LSTM decoder and a decoder dense layer with an activation function\n",
    "    ```\n",
    "    decoder_outputs, state_hidden, state_cell =  decoder_lstm(decoder_inputs,  initial_state=decoder_states_inputs)    \r\n",
    "\r\n",
    "# Saving the new LSTM output stat    es:\r\n",
    "decoder_states = [state_hidden, state_ce    ll]\r\n",
    "\r\n",
    "# Below, we redefine the decoder tput\r\n",
    "# by passing it through the dense     layer:\r\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "  The final setup consist of:\n",
    "    - decoder inputs (the decoder input layer)\n",
    "    - decoder input states (the final states from the encoder)\n",
    "    - decoder outputs (the NumPy matrix we get from the final output layer of the decoder)\n",
    "    - decoder output states (the memory throughout the network from one word to the next)\n",
    "   ```\n",
    "    decoder_model = Model(\r\n",
    "    [decoder_inputs] + decoder_states_inputs,\r\n",
    "    [decoder_outputs] + decoder_states)\n",
    "   ```The actual code below is using a saved HDF5 training model so it will not run.  ```\n",
    "4. \n",
    "work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13feba9-dc5b-4adf-802c-5f6ba93bb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "\n",
    "training_model = load_model('training_model.h5')\n",
    "# These next lines are only necessary\n",
    "# because we're using a saved model:\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "\n",
    "# Building the encoder test model:\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "# Building the two decoder state input layers:\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "\n",
    "# Put the state input layers into a list:\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "\n",
    "# Call the decoder LSTM:\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Redefine the decoder outputs:\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Build the decoder test model:\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919f337-1661-4eba-a154-dce5e9b0a051",
   "metadata": {},
   "source": [
    "### Create the Test Function\n",
    "\n",
    "The function needs to \n",
    "- accept a numpy matrix representing the input sentence\n",
    "- use the encoder and decoder we've created to generate the output\n",
    "- inside the function we'll use the `.predict()` method will take in the new input and it creates output states we can pass to the decoder\n",
    "- then we need an empty array for the output with 3 dimensions(1, 1, num_decoder_tokens)\n",
    "- we know the first value in the output (\"<START>\") so we can giveit into the first timestep\n",
    "- then we need an empty string which will contain the word by word translation - at the end this string will be returned\n",
    "- we’ll decode the sentence word by word using the output state that we retrieved from the encoder (which becomes our decoder’s initial hidden state).\n",
    "- We’ll also update the decoder hidden state after each word so that we use previously decoded words to help decode new ones.\n",
    "- we need a while loop to go word-by-word. It will run till we hit the \"<END>\" string or we hit the maximum sentence legth\n",
    "- inside the loop we use the predict() function to get possible next words and their probabilitites\n",
    "- we'll use numpy's `.argmax()` method to determine the token with the highest probability\n",
    "- we'll add the word to the empty string which will be returned at the end\n",
    "- \n",
    "This block will not run either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4022de6-19c6-4ea1-8e3c-1173a1145e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_model = load_model('training_model.h5')\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def decode_sequence(test_input):\n",
    "  encoder_states_value = encoder_model.predict(test_input)\n",
    "  decoder_states_value = encoder_states_value\n",
    "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "  decoded_sentence = ''\n",
    "  \n",
    "  stop_condition = False\n",
    "  while not stop_condition:\n",
    "    # Run the decoder model to get possible \n",
    "    # output tokens (with probabilities) & states\n",
    "    output_tokens, new_decoder_hidden_state, new_decoder_cell_state = decoder_model.predict([target_seq] + decoder_states_value)\n",
    "\n",
    "    # Choose token with highest probability\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    # Use sampled_token_index to find our next word in reverse_target_features_dict. Save the result to sampled_token\n",
    "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "    decoded_sentence += ' ' + sampled_token\n",
    "    # Exit condition: either hit max length\n",
    "    # or find stop token.\n",
    "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "      stop_condition = True\n",
    "\n",
    "    # Update the target sequence (of length 1).\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    # Update states\n",
    "    decoder_states_value = [new_decoder_hidden_state, new_decoder_cell_state]\n",
    "\n",
    "  return decoded_sentence\n",
    "\n",
    "for seq_index in range(10):\n",
    "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(test_input)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_docs[seq_index])\n",
    "  print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394535d-a9c1-4cb3-b2fc-1bf67022eada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9db1b-d173-4dd6-8126-692c003d51cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
