{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b66af5-963d-4b02-8e2a-67f06f00b9f4",
   "metadata": {},
   "source": [
    "### Choosing the Right Dataset\n",
    " customer service, twitter, slack, reddit, movie data\n",
    "\n",
    " https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    " whatever the data is we need to clean it and create sequence pairs, like dialog or sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18ea37-1cb5-49a2-b8d8-4e76ed7a1f8f",
   "metadata": {},
   "source": [
    "### Setting up the Bot\n",
    "\n",
    "Just as we built a chatbot class to handle the methods for our rule-based and retrieval-based chatbots, we‚Äôll build a chatbot class for our generative chatbot.\r\n",
    "\r\n",
    "Inside, we‚Äôll add a greeting method and a set of exit commands, just like we did for our closed-domain chatbots.\r\n",
    "\r\n",
    "However, in this case, we‚Äôll also import the seq2seq model we‚Äôve built and trained on chat data for you, as well as other information we‚Äôll need to generate a response.\r\n",
    "\r\n",
    "As it happens, many cutting-edge chatbots blend a combination of rule-based, retrieval-based, and generative approaches in order to easily handle some intents using predefined responses and offload other inputs to a natural language generation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7d9f0-751e-48d9-b4a4-5a24c1a17ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "  \n",
    "  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
    "\n",
    "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "  \n",
    "  def start_chat(self):\n",
    "    user_response = input(\"Hi, I'm a chatbot trained on dialog from The Princess Bride. Would you like to chat with me?\\n\")\n",
    "    \n",
    "    if user_response in self.negative_responses:\n",
    "      print(\"Ok, have a great day!\")\n",
    "      return\n",
    "    \n",
    "    self.chat(user_response)\n",
    "  \n",
    "  def chat(self, reply):\n",
    "    while not self.make_exit(reply):\n",
    "      # change this line below:\n",
    "      reply = input(self.generate_response(reply))\n",
    "    \n",
    "  # define .generate_response():\n",
    "  def generate_response(self, user_input):\n",
    "      encoder_states_value = encoder_model.predict(test_input)\n",
    "      decoder_states_value = encoder_states_value\n",
    "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "      target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "      decoded_sentence = ''\n",
    "  \n",
    "      stop_condition = False\n",
    "      while not stop_condition:\n",
    "          # Run the decoder model to get possible \n",
    "          # output tokens (with probabilities) & states\n",
    "              output_tokens, new_decoder_hidden_state, new_decoder_cell_state = decoder_model.predict([target_seq] + decoder_states_value)\n",
    "\n",
    "            # Choose token with highest probability\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            # Use sampled_token_index to find our next word in reverse_target_features_dict. Save the result to sampled_token\n",
    "            sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop token.\n",
    "            if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "              stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "            target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "            # Update states\n",
    "            decoder_states_value = [new_decoder_hidden_state, new_decoder_cell_state]\n",
    "\n",
    "        return decoded_sentence\n",
    "  \n",
    "  \n",
    "  def make_exit(self, reply):\n",
    "    for exit_command in self.exit_commands:\n",
    "      if exit_command in reply:\n",
    "        print(\"Ok, have a great day!\")\n",
    "        return True\n",
    "      \n",
    "    return False\n",
    "  \n",
    "# instantiate your ChatBot below:\n",
    "test = ChatBot()\n",
    "response = test.generate_response('Hi')\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56526e2a-284d-4476-9244-f91bace8c769",
   "metadata": {},
   "source": [
    "we will use the seq2seq model in the generate_response function. \n",
    "If we just paste our .generate_response() method from the seq2seq model it only works with preprocessed data that‚Äôs been converted into a NumPy matrix of one-hot vectors. That won‚Äôt do for our chatbot; we don‚Äôt just want to use test data for our output. We want the .generate_response() method to accept new user input.\r\n",
    "\r\n",
    "Luckily, we can address this by building a method that translates user input into a NumPy matrix. Then we can call that method inside .generate_response() on our user inpu\n",
    "\n",
    "Necessary changes:\n",
    "1. \n",
    "In ChatBot, above .generate_response():- \n",
    "\r\n",
    "Define a new method called .string_to_matrix() with self and user_input as parameter- s.\r\n",
    "Inside .string_to_matrix(), split up the user input string by setting tokens equal to re.findall(r\"[\\w']+|[^\\s\\w]\", user_inpu- t).\r\n",
    "Next, generate a NumPy matrix of zeros called user_input_mat```\n",
    "rix:\r\n",
    "user_input_matrix = np.zeros(\r\n",
    "  (1, max_encoder_seq_length, num_encoder_tokens),\r\n",
    "  dtype='fl```o- at32')\r\n",
    "\r\n",
    "Return the user_input_matrix NumPy matrix from th, t y again\r\n",
    "2.\r\n",
    "Between where you created user_input_matrix and where you returned it from .string_to_matrix(), iterate over each timestep (which is really the index) and token in enumerte(t`okens).\r\n",
    "\r\n",
    "Set user_input_matrix[0, timestep, input_featur`es_dict[token]] to 1.. This creates a one-hot vector within the matrix for each word in \n",
    "\n",
    "3. At the top of the .generate_response() method, convert user_input into a NumPy matrix using self.string_to_matrix().\r",
    "\r\n",
    "Pass the resulting user input matrix into encoder_model.predict() instead of the user_input strin\n",
    "\n",
    "I need to create a py file with the necessary functions to import here, currently the cell below will not workg.the user input.t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcad7d0-5046-4529-9756-e5e7b1629ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from seq2seq import encoder_model, decoder_model, num_decoder_tokens, num_encoder_tokens, input_features_dict, target_features_dict, reverse_target_features_dict, max_decoder_seq_length, max_encoder_seq_length\n",
    "\n",
    "class ChatBot:\n",
    "  \n",
    "  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
    "\n",
    "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "  \n",
    "  def start_chat(self):\n",
    "    user_response = input(\"Hi, I'm a chatbot trained on dialog from The Princess Bride. Would you like to chat with me?\\n\")\n",
    "    \n",
    "    if user_response in self.negative_responses:\n",
    "      print(\"Ok, have a great day!\")\n",
    "      return\n",
    "    \n",
    "    self.chat(user_response)\n",
    "  \n",
    "  def chat(self, reply):\n",
    "    while not self.make_exit(reply):\n",
    "      reply = input(self.generate_response(reply))\n",
    "    \n",
    "  # define .string_to_matrix() below:\n",
    "  def string_to_matrix(self, user_input):\n",
    "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
    "    user_input_matrix = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype = \"float32\")\n",
    "    for timestep, token in enumerate(tokens):\n",
    "      user_input_matrix[0, timestep, input_features_dict[token]] = 1\n",
    "\n",
    "    return user_input_matrix\n",
    "  \n",
    "  def generate_response(self, user_input):\n",
    "    # change user_input into a NumPy matrix:\n",
    "    input_matrix = self.string_to_matrix(user_input)\n",
    "    # update argument for .predict():\n",
    "    states_value = encoder_model.predict(input_matrix)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    \n",
    "    chatbot_response = ''\n",
    "\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "      output_tokens, hidden_state, cell_state = decoder_model.predict(\n",
    "        [target_seq] + states_value)\n",
    "\n",
    "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "      sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "      chatbot_response += \" \" + sampled_token\n",
    "      \n",
    "      if (sampled_token == '<END>' or len(chatbot_response) > max_decoder_seq_length):\n",
    "        stop_condition = True\n",
    "        \n",
    "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "      target_seq[0, 0, sampled_token_index] = 1.\n",
    "      \n",
    "      states_value = [hidden_state, cell_state]\n",
    "      \n",
    "    return chatbot_response\n",
    "  \n",
    "  def make_exit(self, reply):\n",
    "    for exit_command in self.exit_commands:\n",
    "      if exit_command in reply:\n",
    "        print(\"Ok, have a great day!\")\n",
    "        return True\n",
    "      \n",
    "    return False\n",
    "  \n",
    "chatty_mcchatface = ChatBot()\n",
    "# call .generate_response():\n",
    "test = chatty_mcchatface.generate_response('What?')\n",
    "print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f90320-4471-4d2b-9fe2-182c1d26e360",
   "metadata": {},
   "source": [
    "### Handling Unknown Words\r\n",
    "Nice work! Our chatbot now knows how to accept user input. But there is a pretty large caveat here: our chatbot only knows the vocabulary from our training data. What if a user uses a word that the chatbot has never seen before?\r\n",
    "\r\n",
    "With our current code, we‚Äôll get a KeyError:\r\n",
    "\r\n",
    "This is because in .string_to_matrix() we are looking for token in input_features_d```ict:\r\n",
    "\r\n",
    "for timestep, token in enumerate(tokens):\r\n",
    "  user_input_matrix[0, timestep, input_features_dict[token```]] = 1.\r\n",
    "\r\n",
    "Currently, if the token doesn‚Äôt exist in the input_features_dict dictionary (which keeps track of all words in the training data), our program has no way of handling it.\r\n",
    "\r\n",
    "Here are a few popular approaches to tackle unk- own words:\r\n",
    "\r\n",
    "Tell the chatbot to ignore them, which is the simplest fix for smaller datasets, but can never generate those words as output. (Can you imagine scenarios when this could b- e a problem?)\r\n",
    "Pause the chat process and have the chatbot ask what the entire utterance means. This requires the user to rephrase the entire utterance. This causes issues when working with a fairly limited dataset, since we may end up with the chatbot repeatedly asking the user to rephrase each in- put statement.\r\n",
    "Add in a step for the chabot to register any unknown word as a '<UNK>' token. This is generally more complicated than the other two solutions. It would require that the training data is built out with '<UNK>' tokens and requires several ext\n",
    "\n",
    "\n",
    "\r\n",
    "Because we‚Äôre using a tiny dataset to train our model, we‚Äôre going to just ignore words that haven‚Äôt been added into input_features_dict during training.\r\n",
    "\r\n",
    "Fortunately, a simple if statement is all we need.\r\n",
    "\r\n",
    "Add an if clause that checks that token is in input_features_dict so that a one-hot vector only gets created for a know:\n",
    "`if token in input_features_dict.keys():if token in input_features_dict.keys():`\n",
    "\n",
    "\n",
    "### Hooray! \n",
    "While our chatbot is far from perfect, it does produce innovative babble, allowing us to create a truly open-domain conversation. Just like with a machine translation model, we want to accommodate new sentence structures and creativity, something we can do with a generative model for our chatbot.\r\n",
    "\r\n",
    "Of course, even with a better trained model, there are some issues that we need to consider.\r\n",
    "\r\n",
    "First, there are ethical considerations that are much harder to discern and track when working with deep learning models. It‚Äôs very easy to accidentally train a model to reproduce latent biases within the training data.\r\n",
    "\r\n",
    "We‚Äôre also making a pretty big assumption right now in our chatbot architecture: the chatbot responses only depend on the previous turn of user input. The seq2seq model we have here won‚Äôt account for any previous dialog. For example, our chatbot has no way to handle a simple back-and-forth like ```this:\r\n",
    "\r\n",
    "User: ‚ÄúDo you have any siblings?‚Äù\r\n",
    "\r\n",
    "Chatbot: ‚ÄúYes, I do‚Äù\r\n",
    "\r\n",
    "User: ‚ÄúHow many?‚Äù\r\n",
    "```\r\n",
    "Chatbot: ü§Ø\r\n",
    "\r\n",
    "In addition to topics that have been previously covered, the entire context of the chat is missing. Our chatbot doesn‚Äôt know anything about the user, their likes, their dislikes, etc.\r\n",
    "\r\n",
    "As it happens, handling context and previously covered topics is an active area of research in NLP. Some proposed s- lutions include:\r\n",
    "\r\n",
    "training the model to hang onto some previous num- ber of dialog turns\r\n",
    "keeping track of the decoder‚Äôs hidden state-  across dialog turns\r\n",
    "personalizing models by including user context during training or adding user context as it is incl\n",
    "\n",
    "The final code is below.\n",
    "I'll recreate it in VS Code with the functions in separate files.\n",
    "I need to find a way to collect, clean and store the dialogs efficiently - sqlite?\n",
    "I need to find a way to train the model incrementally\n",
    "I need to find out how to add dialogs to the existing data and retrain the modeluded in the user input word:ra manual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dce23-ea2f-4206-963f-3991417a3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from seq2seq import encoder_model, decoder_model, num_decoder_tokens, num_encoder_tokens, input_features_dict, target_features_dict, reverse_target_features_dict, max_decoder_seq_length, max_encoder_seq_length\n",
    "\n",
    "class ChatBot:\n",
    "  \n",
    "  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
    "\n",
    "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "  \n",
    "  def start_chat(self):\n",
    "    user_response = input(\"Hi, I'm a chatbot trained on dialog from The Princess Bride. Would you like to chat with me?\\n\")\n",
    "    \n",
    "    if user_response in self.negative_responses:\n",
    "      print(\"Ok, have a great day!\")\n",
    "      return\n",
    "    \n",
    "    self.chat(user_response)\n",
    "  \n",
    "  def chat(self, reply):\n",
    "    while not self.make_exit(reply):\n",
    "      reply = input(self.generate_response(reply))\n",
    "    \n",
    "  def string_to_matrix(self, user_input):\n",
    "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
    "    user_input_matrix = np.zeros(\n",
    "      (1, max_encoder_seq_length, num_encoder_tokens),\n",
    "      dtype='float32')\n",
    "    for timestep, token in enumerate(tokens):\n",
    "      if token in input_features_dict:\n",
    "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
    "    return user_input_matrix\n",
    "  \n",
    "  def generate_response(self, user_input):\n",
    "    input_matrix = self.string_to_matrix(user_input)\n",
    "    states_value = encoder_model.predict(input_matrix)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    \n",
    "    chatbot_response = ''\n",
    "\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "      output_tokens, hidden_state, cell_state = decoder_model.predict(\n",
    "        [target_seq] + states_value)\n",
    "      \n",
    "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "      sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "      \n",
    "      chatbot_response += \" \" + sampled_token\n",
    "      \n",
    "      if (sampled_token == '<END>' or len(chatbot_response) > max_decoder_seq_length):\n",
    "        stop_condition = True\n",
    "        \n",
    "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "      target_seq[0, 0, sampled_token_index] = 1.\n",
    "      \n",
    "      states_value = [hidden_state, cell_state]\n",
    "      \n",
    "    chatbot_response = chatbot_response.replace(\"<START>\", \"\").replace(\"<END>\", \"\")\n",
    "      \n",
    "    return chatbot_response\n",
    "  \n",
    "  def make_exit(self, reply):\n",
    "    for exit_command in self.exit_commands:\n",
    "      if exit_command in reply:\n",
    "        print(\"Ok, have a great day!\")\n",
    "        return True\n",
    "      \n",
    "    return False\n",
    "  \n",
    "chatty_mcchatface = ChatBot()\n",
    "chatty_mcchatface.start_chat()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d90457-505c-4f51-8aad-ac41f23ac4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfebb0b-8ee5-49ea-b9a7-33ba26a54e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca9c42-ba08-459d-9dd2-ae2d371fa8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41a2bd-1135-4448-a6c5-a3b605a18677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
