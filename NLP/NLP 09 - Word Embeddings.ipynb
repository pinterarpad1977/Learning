{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc3cb7c",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "This idea that a word’s meaning can be understood by its context, or the words that surround it, is the basis for word embeddings. A word embedding is a representation of a word as a numeric vector, enabling us to compare and contrast how words are used and identify words that occur in similar contexts.\n",
    "\n",
    "The applications of word embeddings include:\n",
    "- entity recognition in chatbots\n",
    "- sentiment analysis\n",
    "- syntax parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059b08b",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "Vectors can be many things in many different fields, but ultimately they are containers of information. Depending on the size, or the dimension, of a vector, it can hold varying amounts of data.\n",
    "\n",
    "The simplest case is a 1-dimensional vector, which stores a single number. Say we want to represent the length of a word with a vector. We can do so as follows:\n",
    "\n",
    "\"cat\" ----> [3]\n",
    "\"scrabble\" ----> [8]\n",
    "\"antidisestablishmentarianism\" ----> [28]\n",
    "\n",
    "Instead of looking at these three words with our own eyes, we can compare the vectors that represent them by plotting the vectors on a number line.one-dimensional number line with vectorsWe can clearly see that the “cat” vector is much smaller than the “scrabble” vector, which is much smaller than the “antidisestablishmentarianism” vector.\n",
    "\n",
    "Now let’s say we also want to record the number of vowels in our words, in addition to the number of letters. We can do so using a 2-dimensional vector, where the first entry is the length of the word, and the second entry is the number of vowels:\n",
    "\n",
    "\"cat\" ----> [3, 1]\n",
    "\"scrabble\" ----> [8, 2]\n",
    "\"antidisestablishmentarianism\" ----> [28, 11]\n",
    "\n",
    "To help visualize these vectors, we can plot them on a two-dimensional grid, where the x-axis is the number of letters, and the y-axis is the number of vowels.two-dimensional grid with vectors\n",
    "\n",
    "Here we can see that the vectors for “cat” and “scrabble” point to a more similar area of the grid than the vector for “antidisestablishmentarianism”. So we could argue that “cat” and “scrabble” are closer together.\n",
    "\n",
    "While we have shown here only 1-dimensional and 2-dimensional vectors, we are able to have vectors in any number of dimensions. Even 1,000! The tricky part, however, is visualizing them.\n",
    "\n",
    "Vectors are useful since they help us summarize information about an object using numbers. Then, using the number representation, we can make comparisons between the vector representations of different objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1716a4",
   "metadata": {},
   "source": [
    "### What is a Word Embedding?\n",
    "Word embeddings are vector representations of a word.\n",
    "They allow us to take all the information that is stored in a word, like its meaning and its part of speech, and convert it into a numeric form that is more understandable to a computer.\n",
    "\n",
    "For example, we could look at a word embedding for the word “peace”.\n",
    "\n",
    "[5.2907305, -4.20267, 1.6989858, -1.422668, -1.500128, ...]\n",
    "\n",
    "Here “peace” is represented by a 96-dimension vector, with just the first five dimensions shown. Each dimension of the vector is capturing some information about how the word “peace” is used. We can also look at a word embedding for the word “war”:\n",
    "\n",
    "[7.2966490, -0.52887750, 0.97479630, -2.9508233, -3.3934135, ...]\n",
    "\n",
    "By converting the words “war” and “peace” into their numeric vector representations, we are able to have a computer more easily compare the vectors and understand their similarities and differences.\n",
    "\n",
    "We can load a basic English word embedding model using `spaCy` as follows:\n",
    "\n",
    "`nlp = spacy.load('en')`\n",
    "\n",
    "**Note**: the convention is to load spaCy models into a variable named nlp.\n",
    "\n",
    "To get the vector representation of a word, we call the model with the desired word as an argument and can use the .vector attribute.\n",
    "\n",
    "`nlp('love').vector`\n",
    "\n",
    "But how do we compare these vectors? And how do we arrive at these numeric representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634dd501",
   "metadata": {},
   "source": [
    "### Distance\n",
    "The key at the heart of word embeddings is distance. Before we explain why, let’s dive into how the distance between vectors can be measured. \n",
    "When working with vectors that have a large number of dimensions, such as word embeddings, the distances calculated by Manhattan and Euclidean distance can become rather large. Thus, calculations using cosine distance are preferred!\n",
    "\n",
    "**Cosine distance** is concerned with **the angle between two vectors**, rather than by looking at the distance between the points, or ends, of the vectors. Two vectors that point in the same direction have no angle between them, and have a cosine distance of 0. Two vectors that point in opposite directions, on the other hand, have a cosine distance of 1.\n",
    "\n",
    "We can easily calculate the Manhattan, Euclidean, and cosine distances between vectors using helper functions from SciPy:\n",
    "```\n",
    "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
    "\n",
    "vector_a = np.array([1,2,3])\n",
    "vector_b = np.array([2,4,6])\n",
    "\n",
    "# Manhattan distance:\n",
    "manhattan_d = cityblock(vector_a,vector_b) # 6\n",
    "\n",
    "# Euclidean distance:\n",
    "euclidean_d = euclidean(vector_a,vector_b) # 3.74\n",
    "\n",
    "# Cosine distance:\n",
    "cosine_d = cosine(vector_a,vector_b) # 0.0\n",
    "```\n",
    "\n",
    "The idea behind word embeddings is a theory known as the **distributional hypothesis**. This hypothesis states that words that co-occur in the same contexts tend to have similar meanings. With word embeddings, we map words that exist with the same context to similar places in our vector space (math-speak for the area in which our vectors exist).\n",
    "\n",
    "The numeric values that are assigned to the vector representation of a word are not important in their own right, but gather meaning from how similar or not words are to each other.\n",
    "\n",
    "Thus the cosine distance between words with similar contexts will be small, and the cosine distance between words that have very different contexts will be large.\n",
    "\n",
    "The literal values of a word’s embedding have no actual meaning. We gain value in word embeddings from comparing the different word vectors and seeing how similar or different they are.\n",
    "\n",
    "**I am just saving this function here which will provide the closest 10 words to a word based on a word list and a vector list**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8225d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_words(word_list, vector_list, word_to_check):\n",
    "    return sorted(word_list,\n",
    "                  key=lambda x: cosine(vector_list[word_list.index(word_to_check)], vector_list[word_list.index(x)]))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6d56a",
   "metadata": {},
   "source": [
    "### Word2vec\n",
    "You might be asking yourself a question now. How did we arrive at the vector values that define a word vector? And how do we ensure that the values chosen place the vectors for words with similar context close together and the vectors for words with different usages far apart?\n",
    "\n",
    "Step in word2vec! Word2vec is a statistical learning algorithm that develops word embeddings from a corpus of text. Word2vec uses one of two different model architectures to come up with the values that define a collection of word embeddings.\n",
    "\n",
    "One method is to use the **continuous bag-of-words (CBOW)** representation of a piece of text. The word2vec model goes through each word in the training corpus, in order, and tries to predict what word comes at each position based on applying bag-of-words to the words that surround the word in question. In this approach, **the order of the words does not matter**!\n",
    "\n",
    "The other method word2vec can use to create word embeddings is continuous **skip-grams**. Skip-grams function similarly to n-grams, except instead of looking at groupings of n-consecutive words in a text, we can look at sequences of words that are separated by some specified distance between them.\n",
    "\n",
    "For example, consider the sentence \"The squids jump out of the suitcase\". The 1-skip-2-grams includes all the bigrams (2-grams) as well as the following subsequences:\n",
    "\n",
    "(The, jump), (squids, out), (jump, of), (out, the), (of, suitcase)\n",
    "\n",
    "When using continuous skip-grams, **the order of context is taken into consideration**! Because of this, the time it takes to train the word embeddings is slower than when using continuous bag-of-words. The results, however, are often much better!\n",
    "\n",
    "With either the continuous bag-of-words or continuous skip-grams representations as training data, word2vec then uses a shallow, 2-layer neural network to come up with the values that place words with a similar context in vectors near each other and words with different contexts in vectors far apart from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c415ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence = \"It was the best of times, it was the worst of times.\"\n",
    "print(sentence)\n",
    "\n",
    "# preprocessing\n",
    "sentence_lst = [word.lower().strip(\".\") for word in sentence.split()]\n",
    "\n",
    "# set context_length\n",
    "context_length = 3\n",
    "\n",
    "# function to get cbows\n",
    "def get_cbows(sentence_lst, context_length):\n",
    "  cbows = list()\n",
    "  for i, val in enumerate(sentence_lst):\n",
    "    if i < context_length:\n",
    "      pass\n",
    "    elif i < len(sentence_lst) - context_length:\n",
    "      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
    "      vectorizer = CountVectorizer()\n",
    "      vectorizer.fit_transform(context)\n",
    "      context_no_order = vectorizer.get_feature_names()\n",
    "      cbows.append((val,context_no_order))\n",
    "  return cbows\n",
    "\n",
    "# define cbows here:\n",
    "cbows = get_cbows(sentence_lst, context_length)\n",
    "\n",
    "# function to get cbows\n",
    "def get_skip_grams(sentence_lst, context_length):\n",
    "  skip_grams = list()\n",
    "  for i, val in enumerate(sentence_lst):\n",
    "    if i < context_length:\n",
    "      pass\n",
    "    elif i < len(sentence_lst) - context_length:\n",
    "      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
    "      skip_grams.append((val, context))\n",
    "  return skip_grams\n",
    "\n",
    "# define skip_grams here:\n",
    "skip_grams = get_skip_grams(sentence_lst, context_length)\n",
    "\n",
    "try:\n",
    "  print('\\nContinuous Bag of Words')\n",
    "  for cbow in cbows:\n",
    "    print(cbow)\n",
    "except:\n",
    "  pass\n",
    "try:\n",
    "  print('\\nSkip Grams')\n",
    "  for skip_gram in skip_grams:\n",
    "    print(skip_gram)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef53b68",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "Depending on the corpus of text we select to train a word embedding model, different word embeddings will be created according to the context of the words in the given corpus. The larger and more generic a corpus, however, the more generalizable the word embeddings become.\n",
    "\n",
    "When we want to train our own word2vec model on a corpus of text, we can use the gensim package!\n",
    "\n",
    "In previous exercises, we have been using pre-trained word embedding models stored in spaCy. These models were trained, using word2vec, on blog posts and news articles collected by the Linguistic Data Consortium at the University of Pennsylvania. With gensim, however, we are able to build our own word embeddings on any corpus of text we like.\n",
    "\n",
    "To easily train a word2vec model on our own corpus of text, we can use gensim’s Word2Vec() function.\n",
    "\n",
    "`model = gensim.models.Word2Vec(corpus, size=100, window=5, min_count=1, workers=2, sg=1)`\n",
    "- corpus is a list of lists, where each inner list is a document in the corpus and each element in the inner lists is a word token\n",
    "- size determines how many dimensions our word embeddings will include. Word embeddings often have upwards of 1,000 dimensions! Here we will create vectors of 100-dimensions to keep things simple.\n",
    "- don’t worry about the rest of the keyword arguments here!\n",
    "\n",
    "To view the entire vocabulary used to train the word embedding model, we can use the `.wv.vocab.items()` method.\n",
    "\n",
    "`vocabulary_of_model = list(model.wv.vocab.items())`\n",
    "\n",
    "When we train a word2vec model on a smaller corpus of text, we pick up on the unique ways in which words of the text are used.\n",
    "\n",
    "For example, if we were using scripts from the television show Friends as a training corpus, the model would pick up on the unique ways in which words are used in the show. While the generalized vectors in a spaCy model might not place the vectors for “Ross” and “Rachel” close together, a gensim word embedding model trained on Friends’ scripts would place the vectors for words like “Ross” and “Rachel”, two characters that have a continuous on and off-again relationship throughout the show, very close together!\n",
    "\n",
    "To easily find which vectors gensim placed close together in its word embedding model, we can use the .most_similar() method.\n",
    "\n",
    "`model.most_similar(\"my_word_here\", topn=100)`\n",
    "\n",
    "\"my_word_here\" is the target word token we want to find most similar words to\n",
    "topn is a keyword argument that indicates how many similar word vectors we want returned\n",
    "One last gensim method we will explore is a rather fun one: `.doesnt_match()`.\n",
    "\n",
    "`model.doesnt_match([\"asia\", \"mars\", \"pluto\"])`\n",
    "\n",
    "when given a list of terms in the vocabulary as an argument, .doesnt_match() returns which term is furthest from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77b66ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n",
      "[('romeo.', 0.9952322244644165), ('mercutio.', 0.9949089288711548), ('juliet.', 0.9934136271476746), ('nurse.', 0.9932466745376587), ('love', 0.9931928515434265), ('doth', 0.992635190486908), ('benvolio.', 0.9925016760826111), ('good', 0.9924439191818237), ('i’ll', 0.9923396110534668), ('capulet.', 0.9918347001075745), ('one', 0.9916640520095825), ('art', 0.9916371703147888), ('romeo,', 0.9916179180145264), ('come', 0.9914699792861938), ('love,', 0.9912819862365723), ('fair', 0.99127197265625), ('thee', 0.9912043809890747), ('take', 0.991066575050354), ('hath', 0.9910546541213989), ('say', 0.9896630644798279)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import import_ipynb\n",
    "from romeo_juliet import romeo_and_juliet\n",
    "\n",
    "# load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['thou', 'shall', 'like', 'thy', 'would', 'may', 'o,', '’tis', 'yet']\n",
    "print(stop_words[:5])\n",
    "# preprocess text\n",
    "romeo_and_juliet_processed = [[word for word in romeo_and_juliet.lower().split() if word not in stop_words]]\n",
    "\n",
    "# view inner list of romeo_and_juliet_processed\n",
    "#print(romeo_and_juliet_processed[0][:20])\n",
    "\n",
    "# train word embeddings model\n",
    "model = gensim.models.Word2Vec(romeo_and_juliet_processed, vector_size = 100, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# view vocabulary\n",
    "vocabulary = list(model.wv.key_to_index.items())\n",
    "#print(vocabulary)\n",
    "\n",
    "# similar to romeo\n",
    "similar_to_romeo = model.wv.similar_by_word('romeo', topn=20)\n",
    "print(similar_to_romeo)\n",
    "\n",
    "# one is not like the others\n",
    "not_star_crossed_lover = model.wv.doesnt_match(['romeo', 'juliet', 'mercutio'])\n",
    "#print(not_star_crossed_lover)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee345e",
   "metadata": {},
   "source": [
    "### Review\n",
    "Lost in a multidimensional vector space after this lesson? We hope not! We have covered a lot here, so let’s take some time to recap.\n",
    "- Vectors are containers of information, and they can have anywhere from 1-dimension to hundreds or thousands of dimensions\n",
    "- Word embeddings are vector representations of a word, where words with similar contexts are represented with vectors that are closer together\n",
    "- spaCy is a package that enables us to view and use pre-trained word embedding models\n",
    "- The distance between vectors can be calculated in many ways, and the best way for measuring the distance between higher dimensional vectors is cosine distance\n",
    "- Word2Vec is a shallow neural network model that can build word embeddings using either continuous bag-of-words or continuous skip-grams\n",
    "- Gensim is a package that allows us to create and train word embedding models using any corpus of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8eeb6",
   "metadata": {},
   "source": [
    "### Token.similarity | spaCy\n",
    "https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc46a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
