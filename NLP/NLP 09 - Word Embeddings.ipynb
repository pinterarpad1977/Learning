{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc3cb7c",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "This idea that a word’s meaning can be understood by its context, or the words that surround it, is the basis for word embeddings. A word embedding is a representation of a word as a numeric vector, enabling us to compare and contrast how words are used and identify words that occur in similar contexts.\n",
    "\n",
    "The applications of word embeddings include:\n",
    "- entity recognition in chatbots\n",
    "- sentiment analysis\n",
    "- syntax parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059b08b",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "Vectors can be many things in many different fields, but ultimately they are containers of information. Depending on the size, or the dimension, of a vector, it can hold varying amounts of data.\n",
    "\n",
    "The simplest case is a 1-dimensional vector, which stores a single number. Say we want to represent the length of a word with a vector. We can do so as follows:\n",
    "\n",
    "\"cat\" ----> [3]\n",
    "\"scrabble\" ----> [8]\n",
    "\"antidisestablishmentarianism\" ----> [28]\n",
    "\n",
    "Instead of looking at these three words with our own eyes, we can compare the vectors that represent them by plotting the vectors on a number line.one-dimensional number line with vectorsWe can clearly see that the “cat” vector is much smaller than the “scrabble” vector, which is much smaller than the “antidisestablishmentarianism” vector.\n",
    "\n",
    "Now let’s say we also want to record the number of vowels in our words, in addition to the number of letters. We can do so using a 2-dimensional vector, where the first entry is the length of the word, and the second entry is the number of vowels:\n",
    "\n",
    "\"cat\" ----> [3, 1]\n",
    "\"scrabble\" ----> [8, 2]\n",
    "\"antidisestablishmentarianism\" ----> [28, 11]\n",
    "\n",
    "To help visualize these vectors, we can plot them on a two-dimensional grid, where the x-axis is the number of letters, and the y-axis is the number of vowels.two-dimensional grid with vectors\n",
    "\n",
    "Here we can see that the vectors for “cat” and “scrabble” point to a more similar area of the grid than the vector for “antidisestablishmentarianism”. So we could argue that “cat” and “scrabble” are closer together.\n",
    "\n",
    "While we have shown here only 1-dimensional and 2-dimensional vectors, we are able to have vectors in any number of dimensions. Even 1,000! The tricky part, however, is visualizing them.\n",
    "\n",
    "Vectors are useful since they help us summarize information about an object using numbers. Then, using the number representation, we can make comparisons between the vector representations of different objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1716a4",
   "metadata": {},
   "source": [
    "### What is a Word Embedding?\n",
    "Word embeddings are vector representations of a word.\n",
    "They allow us to take all the information that is stored in a word, like its meaning and its part of speech, and convert it into a numeric form that is more understandable to a computer.\n",
    "\n",
    "For example, we could look at a word embedding for the word “peace”.\n",
    "\n",
    "[5.2907305, -4.20267, 1.6989858, -1.422668, -1.500128, ...]\n",
    "\n",
    "Here “peace” is represented by a 96-dimension vector, with just the first five dimensions shown. Each dimension of the vector is capturing some information about how the word “peace” is used. We can also look at a word embedding for the word “war”:\n",
    "\n",
    "[7.2966490, -0.52887750, 0.97479630, -2.9508233, -3.3934135, ...]\n",
    "\n",
    "By converting the words “war” and “peace” into their numeric vector representations, we are able to have a computer more easily compare the vectors and understand their similarities and differences.\n",
    "\n",
    "We can load a basic English word embedding model using `spaCy` as follows:\n",
    "\n",
    "`nlp = spacy.load('en')`\n",
    "\n",
    "**Note**: the convention is to load spaCy models into a variable named nlp.\n",
    "\n",
    "To get the vector representation of a word, we call the model with the desired word as an argument and can use the .vector attribute.\n",
    "\n",
    "`nlp('love').vector`\n",
    "\n",
    "But how do we compare these vectors? And how do we arrive at these numeric representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634dd501",
   "metadata": {},
   "source": [
    "### Distance\n",
    "The key at the heart of word embeddings is distance. Before we explain why, let’s dive into how the distance between vectors can be measured. \n",
    "When working with vectors that have a large number of dimensions, such as word embeddings, the distances calculated by Manhattan and Euclidean distance can become rather large. Thus, calculations using cosine distance are preferred!\n",
    "\n",
    "**Cosine distance** is concerned with **the angle between two vectors**, rather than by looking at the distance between the points, or ends, of the vectors. Two vectors that point in the same direction have no angle between them, and have a cosine distance of 0. Two vectors that point in opposite directions, on the other hand, have a cosine distance of 1.\n",
    "\n",
    "We can easily calculate the Manhattan, Euclidean, and cosine distances between vectors using helper functions from SciPy:\n",
    "```\n",
    "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
    "\n",
    "vector_a = np.array([1,2,3])\n",
    "vector_b = np.array([2,4,6])\n",
    "\n",
    "# Manhattan distance:\n",
    "manhattan_d = cityblock(vector_a,vector_b) # 6\n",
    "\n",
    "# Euclidean distance:\n",
    "euclidean_d = euclidean(vector_a,vector_b) # 3.74\n",
    "\n",
    "# Cosine distance:\n",
    "cosine_d = cosine(vector_a,vector_b) # 0.0\n",
    "```\n",
    "\n",
    "The idea behind word embeddings is a theory known as the **distributional hypothesis**. This hypothesis states that words that co-occur in the same contexts tend to have similar meanings. With word embeddings, we map words that exist with the same context to similar places in our vector space (math-speak for the area in which our vectors exist).\n",
    "\n",
    "The numeric values that are assigned to the vector representation of a word are not important in their own right, but gather meaning from how similar or not words are to each other.\n",
    "\n",
    "Thus the cosine distance between words with similar contexts will be small, and the cosine distance between words that have very different contexts will be large.\n",
    "\n",
    "The literal values of a word’s embedding have no actual meaning. We gain value in word embeddings from comparing the different word vectors and seeing how similar or different they are.\n",
    "\n",
    "**I am just saving this function here which will provide the closest 10 words to a word based on a word list and a vector list**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8225d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_words(word_list, vector_list, word_to_check):\n",
    "    return sorted(word_list,\n",
    "                  key=lambda x: cosine(vector_list[word_list.index(word_to_check)], vector_list[word_list.index(x)]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbefea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c415ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18683c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b66ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1727ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69039ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45b5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779e37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192f828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80f839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc50df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc46a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
