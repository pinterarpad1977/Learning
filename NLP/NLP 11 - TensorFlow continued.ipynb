{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360d3032",
   "metadata": {},
   "source": [
    "### Training Setup (part 1)\n",
    "\n",
    "For each sentence, Keras expects a NumPy matrix containing one-hot vectors for each token. What’s a one-hot vector? In a one-hot vector, every token in our set is represented by a 0 except for the current token which is represented by a 1. For example given the vocabulary [\"the\", \"dog\", \"licked\", \"me\"], a one-hot vector for “dog” would look like [0, 1, 0, 0].\n",
    "\n",
    "In order to vectorize our data and later translate it from vectors, it’s helpful to have a features dictionary (and a reverse features dictionary) to easily translate between all the 1s and 0s and actual words. We’ll build out the following:\n",
    "- a features dictionary for English\n",
    "- a features dictionary for Spanish\n",
    "- a reverse features dictionary for English (where the keys and values are swapped)\n",
    "- a reverse features dictionary for Spanish\n",
    "\n",
    "Once we have all of our features dictionaries set up, it’s time to vectorize the data! We’re going to need vectors to input into our encoder and decoder, as well as a vector of target data we can use to train the decoder.\n",
    "\n",
    "Because each matrix is almost all zeros, we’ll use numpy.zeros() from the NumPy library to build them out.\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "```\n",
    "We defined a NumPy matrix of zeros called encoder_input_data with two arguments:\n",
    "- the shape of the matrix — in our case the number of documents (or sentences) by the maximum token sequence length (the longest sentence we want to see) by the number of unique tokens (or words)\n",
    "- the data type we want — in our case NumPy’s float32, which can speed up our processing a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50969f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import re\n",
    "# Importing our translations\n",
    "data_path = \"span-eng.txt\"\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "\n",
    "# Building empty lists to hold sentences\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "# Building empty vocabulary sets\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "for line in lines:\n",
    "  # Input and target sentences are separated by tabs\n",
    "  input_doc, target_doc = line.split('\\t')\n",
    "  # Appending each input sentence to input_docs\n",
    "  input_docs.append(input_doc)\n",
    "  # Splitting words from punctuation\n",
    "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "  # Redefine target_doc below \n",
    "  # and append it to target_docs:\n",
    "  target_doc = '<START> ' + target_doc + ' <END>'\n",
    "  target_docs.append(target_doc)\n",
    "  \n",
    "  # Now we split up each sentence into words\n",
    "  # and add each unique word to our vocabulary set\n",
    "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "    print(token)\n",
    "    # Add your code here:\n",
    "    if token not in input_tokens:\n",
    "      input_tokens.add(token)\n",
    "  for token in target_doc.split():\n",
    "    print(token)\n",
    "    # And here:\n",
    "    if token not in target_tokens:\n",
    "      target_tokens.add(token)\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "# Create num_encoder_tokens and num_decoder_tokens:\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from preprocessing import input_docs, target_docs, input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
    "\n",
    "print('Number of samples:', len(input_docs))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(input_tokens)])\n",
    "# Build out target_features_dict:\n",
    "target_features_dict = dict(\n",
    "  [(token,i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_features_dict = dict(\n",
    "    (i, token) for token, i in input_features_dict.items())\n",
    "# Build out reverse_target_features_dict:\n",
    "reverse_target_features_dict = dict(\n",
    "  (i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "print(\"\\nHere's the first item in the encoder input matrix:\\n\", encoder_input_data[0], \"\\n\\nThe number of columns should match the number of unique input tokens and the number of rows should match the maximum sequence length for input sentences.\")\n",
    "\n",
    "# Build out the decoder_input_data matrix:\n",
    "decoder_input_data = np.zeros(\n",
    "  (len(target_docs), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32')\n",
    "\n",
    "# Build out the decoder_target_data matrix:\n",
    "decoder_target_data = np.zeros(\n",
    "  (len(target_docs), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec0fba",
   "metadata": {},
   "source": [
    "### Training Setup (part 2)\n",
    "At this point we need to fill out the 1s in each vector. We can loop over each English-Spanish pair in our training sample using the features dictionaries to add a 1 for the token in question. For example, the dog sentence ([\"the\", \"dog\", \"licked\", \"me\"]) would be split into the following matrix of vectors:\n",
    "```\n",
    "[\n",
    "  [1, 0, 0, 0], # timestep 0 => \"the\"\n",
    "  [0, 1, 0, 0], # timestep 1 => \"dog\"\n",
    "  [0, 0, 1, 0], # timestep 2 => \"licked\"\n",
    "  [0, 0, 0, 1], # timestep 3 => \"me\"\n",
    "]\n",
    "```\n",
    "You’ll notice the vectors have timesteps — we use these to track where in a given document (sentence) we are.\n",
    "\n",
    "To build out a three-dimensional NumPy matrix of one-hot vectors, we can assign a value of 1 for a given word at a given timestep in a given line:\n",
    "\n",
    "`matrix_name[line, timestep, features_dict[token]] = 1.`\n",
    "\n",
    "Keras will fit — or train — the seq2seq model using these matrices of one-hot vectors:\n",
    "- the encoder input data\n",
    "- the decoder input data\n",
    "- the decoder target data\n",
    "\n",
    "Hang on a second, why build two matrices of decoder data? Aren’t we just encoding and decoding?\n",
    "\n",
    "The reason has to do with a technique known as teacher forcing that most seq2seq models employ during training. Here’s the idea: we have a Spanish input token from the previous timestep to help train the model for the current timestep’s target token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import re\n",
    "from preprocessing import input_docs, target_docs, input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
    "\n",
    "input_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(input_tokens)])\n",
    "target_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_input_features_dict = dict(\n",
    "    (i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict(\n",
    "    (i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "\n",
    "  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "\n",
    "    print(\"Encoder input timestep & token:\", timestep, token)\n",
    "    print(input_features_dict[token])\n",
    "    # Assign 1. for the current line, timestep, & word\n",
    "    # in encoder_input_data:\n",
    "    encoder_input_data[line, timestep, input_features_dict[token]] = 1\n",
    "\n",
    "  for timestep, token in enumerate(target_doc.split()):\n",
    "\n",
    "    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "    print(\"Decoder input timestep & token:\", timestep, token)\n",
    "    # Assign 1. for the current line, timestep, & word\n",
    "    # in decoder_input_data:\n",
    "    decoder_input_data[line, timestep, target_features_dict[token]] = 1\n",
    "    if timestep > 0:\n",
    "      # decoder_target_data is ahead by 1 timestep\n",
    "      # and doesn't include the start token.\n",
    "      print(\"Decoder target timestep:\", timestep)\n",
    "      # Assign 1. for the current line, timestep, & word\n",
    "      # in decoder_target_data:\n",
    "      decoder_target_data[line, timestep-1, target_features_dict[token]] = 1\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7addd380",
   "metadata": {},
   "source": [
    "### Encoder Training Setup\n",
    "It’s time for some deep learning!\n",
    "\n",
    "Deep learning models in Keras are built in layers, where each layer is a step in the model.\n",
    "\n",
    "Our encoder requires two layer types from Keras:\n",
    "\n",
    "An input layer, which defines a matrix to hold all the one-hot vectors that we’ll feed to the model.\n",
    "An LSTM layer, with some output dimensionality.\n",
    "We can import these layers as well as the model we need like so:\n",
    "```\n",
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "```\n",
    "Next, we set up the input layer, which requires some number of dimensions that we’re providing. In this case, we know that we’re passing in all the encoder tokens, but we don’t necessarily know our batch size (how many chocolate chip cookies sentences we’re feeding the model at a time). Fortunately, we can say None because the code is written to handle varying batch sizes, so we don’t need to specify that dimension.\n",
    "```\n",
    "\n",
    "# the shape specifies the input matrix sizes\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "```\n",
    "For the LSTM layer, we need to select the dimensionality (the size of the LSTM’s hidden states, which helps determine how closely the model molds itself to the training data — something we can play around with) and whether to return the state (in this case we do):\n",
    "```\n",
    "encoder_lstm = LSTM(100, return_state=True)\n",
    "# we're using a dimensionality of 100\n",
    "# so any LSTM output matrix will have \n",
    "# shape [batch_size, 100]\n",
    "```\n",
    "Remember, the only thing we want from the encoder is its final states. We can get these by linking our LSTM layer with our input layer:\n",
    "\n",
    "`encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)`\n",
    "\n",
    "encoder_outputs isn’t really important for us, so we can just discard it. However, the states, we’ll save in a list:\n",
    "\n",
    "`encoder_states = [state_hidden, state_cell]`\n",
    "\n",
    "There is a lot to take in here, but there’s no need to memorize any of this — you got this.💪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import num_encoder_tokens\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "# Create the input layer:\n",
    "encoder_inputs = Input( shape = (None, num_encoder_tokens))\n",
    "\n",
    "# Create the LSTM layer:\n",
    "encoder_lstm = LSTM(256, return_state = True)\n",
    "\n",
    "# Retrieve the outputs and states:\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Put the states together in a list:\n",
    "encoder_states = [state_hidden, state_cell]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46158f2",
   "metadata": {},
   "source": [
    "### Decoder Training Setup\n",
    "The decoder looks a lot like the encoder (phew!), with an input layer and an LSTM layer that we use together:\n",
    "```\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(100, return_sequences=True, return_state=True)\n",
    "# This time we care about full return sequences\n",
    "```\n",
    "However, with our decoder, we pass in the state data from the encoder, along with the decoder inputs. This time, we’ll keep the output instead of the states:\n",
    "```\n",
    "# The two states will be discarded for now\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = \n",
    "    decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "```\n",
    "We also need to run the output through a final activation layer, using the Softmax function, that will give us the probability distribution — where all probabilities sum to one — for each token. The final layer also transforms our LSTM output from a dimensionality of whatever we gave it (in our case, 10) to the number of unique words within the hidden layer’s vocabulary (i.e., the number of unique target tokens, which is definitely more than 10!).\n",
    "```\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "```\n",
    "Keras’s implementation could work with several layer types, but Dense is the least complex, so we’ll go with that. We also need to modify our import statement to include it before running the code:\n",
    "\n",
    "`from keras.layers import Input, LSTM, Dense`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import num_encoder_tokens, num_decoder_tokens\n",
    "\n",
    "from tensorflow import keras\n",
    "# Add Dense to the imported layers\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# The decoder input and LSTM layers:\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "\n",
    "# Retrieve the LSTM outputs and states:\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "\n",
    "# Build a final Dense layer:\n",
    "decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
    "\n",
    "# Filter outputs through the Dense layer:\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3baea",
   "metadata": {},
   "source": [
    "### Build and Train seq2seq\n",
    "Alright! Let’s get model-building!\n",
    "\n",
    "First, we define the seq2seq model using the Model() function we imported from Keras. To make it a seq2seq model, we feed it the encoder and decoder inputs, as well as the decoder output:\n",
    "\n",
    "`model = Model([encoder_inputs, decoder_inputs], decoder_outputs)`\n",
    "\n",
    "Finally, our model is ready to train. First, we compile everything. Keras models demand two arguments to compile:\n",
    "- An optimizer (we’re using RMSprop, which is a fancy version of the widely-used gradient descent) to help minimize our error rate (how bad the model is at guessing the true next word given the previous words in a sentence).\n",
    "- loss function (we’re using the logarithm-based cross-entropy function) to determine the error rate.\n",
    "- Because we care about accuracy, we’re adding that into the metrics to pay attention to while training. Here’s what the compiling code looks like:\n",
    "\n",
    "```\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "Next we need to fit the compiled model. To do this, we give the .fit() method the encoder and decoder input data (what we pass into the model), the decoder target data (what we expect the model to return given the data we passed in), and some numbers we can adjust as needed:\n",
    "- batch size (smaller batch sizes mean more time, and for some problems, smaller batch sizes will be better, while for other problems, larger batch sizes are better)\n",
    "- the number of epochs or cycles of training (more epochs mean a model that is more trained on the dataset, and that the process will take more time)\n",
    "- validation split (what percentage of the data should be set aside for validating — and determining when to stop training your model — rather than training)\n",
    "\n",
    "Keras will take it from here to get you a (hopefully) nicely trained seq2seq model:\n",
    "```\n",
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data,\n",
    "          batch_size=10,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)\n",
    "          ```\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c20b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import num_encoder_tokens, num_decoder_tokens, decoder_target_data, encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "from tensorflow import keras\n",
    "# Add Dense to the imported layers\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Encoder training setup\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Decoder training setup:\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Building the training model:\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "print(\"Model summary:\\n\")\n",
    "training_model.summary()\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Compile the model:\n",
    "training_model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Choose the batch size\n",
    "# and number of epochs:\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "print(\"Training the model:\\n\")\n",
    "# Train the model:\n",
    "training_model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2190c",
   "metadata": {},
   "source": [
    "### Setup for Testing\n",
    "Now our model is ready for testing! Yay! However, to generate some original output text, we need to **redefine the seq2seq architecture** in pieces. Wait, didn’t we just define and train a model?\n",
    "\n",
    "Well, yes. But the model we used for training our network only works when we already know the target sequence. This time, we have no idea what the Spanish should be for the English we pass in! So we need a model that will decode step-by-step instead of using teacher forcing. To do this, we need a seq2seq network in individual pieces.\n",
    "\n",
    "To start, we’ll build an encoder model with our encoder inputs and the placeholders for the encoder’s output states:\n",
    "\n",
    "`encoder_model = Model(encoder_inputs, encoder_states)`\n",
    "\n",
    "Next up, we need placeholders for the decoder’s input states, which we can build as input layers and store together. Why? We don’t know what we want to decode yet or what hidden state we’re going to end up with, so we need to do everything step-by-step. We need to pass the encoder’s final hidden state to the decoder, sample a token, and get the updated hidden state back. Then we’ll be able to (manually) pass the updated hidden state back into the network:\n",
    "```\n",
    "latent_dim = 256\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "```\n",
    "Using the decoder LSTM and decoder dense layer (with the activation function) that we trained earlier, we’ll create new decoder states and outputs:\n",
    "```\n",
    "decoder_outputs, state_hidden, state_cell = \n",
    "    decoder_lstm(decoder_inputs, \n",
    "    initial_state=decoder_states_inputs)\n",
    "\n",
    "# Saving the new LSTM output states:\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Below, we redefine the decoder output\n",
    "# by passing it through the dense layer:\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "```\n",
    "Finally, we can set up the decoder model. This is where we bring together:\n",
    "- the decoder inputs (the decoder input layer)\n",
    "- the decoder input states (the final states from the encoder)\n",
    "- the decoder outputs (the NumPy matrix we get from the final output layer of the decoder)\n",
    "- the decoder output states (the memory throughout the network from one word to the next)\n",
    "```\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "```\n",
    "\n",
    "As you may have noticed, we moved everything to another file again. We also saved the training model on its own as an HDF5 file, which we are loading back up in script.py.\n",
    "https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "\n",
    "We already created the encoder test model and built two decoder state input layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "training_model = load_model('training_model.h5')\n",
    "# These next lines are only necessary\n",
    "# because we're using a saved model:\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "\n",
    "# Building the encoder test model:\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "# Building the two decoder state input layers:\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "\n",
    "# Put the state input layers into a list:\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "\n",
    "# Call the decoder LSTM:\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "\n",
    "# Redefine the decoder outputs:\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Build the decoder test model:\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55fe72",
   "metadata": {},
   "source": [
    "### The Test Function\n",
    "Finally, we can get to testing our model! To do this, we need to build a function that:\n",
    "- accepts a NumPy matrix representing the test English sentence input\n",
    "- uses the encoder and decoder we’ve created to generate Spanish output\n",
    "\n",
    "Inside the test function, we’ll run our new English sentence through the encoder model. The .predict() method takes in new input (as a NumPy matrix) and gives us output states that we can pass on to the decoder:\n",
    "```\n",
    "# test_input is a NumPy matrix\n",
    "# representing an English sentence\n",
    "states = encoder.predict(test_input)\n",
    "```\n",
    "Next, we’ll build an empty NumPy array for our Spanish translation, giving it three dimensions:\n",
    "```\n",
    "# batch size: 1\n",
    "# number of tokens to start with: 1\n",
    "# number of tokens in our target vocabulary\n",
    "target_sequence = np.zeros((1, 1, num_decoder_tokens))\n",
    "```\n",
    "Luckily, we already know the first value in our Spanish sentence — \"<Start>\"! So we can give \"<Start>\" a value of 1 at the first timestep:\n",
    "\n",
    "`target_sequence[0, 0, target_features_dict['<START>']] = 1.`\n",
    "\n",
    "Before we get decoding, we’ll need a string where we can add our translation to, word by word:\n",
    "\n",
    "`decoded_sentence = ''`\n",
    "\n",
    "This is the variable that we will ultimately return from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d048681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens\n",
    "\n",
    "from prep import target_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, target_tokens\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np\n",
    "\n",
    "training_model = load_model('training_model.h5')\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def decode_sequence(test_input):\n",
    "  # Encode the input as state vectors:\n",
    "  encoder_states_value = encoder_model.predict(test_input)\n",
    "  # Set decoder states equal to encoder final states\n",
    "  # Because we’re transferring the states from the encoder to the decoder\n",
    "  decoder_states_value = encoder_states_value\n",
    "  # Generate empty target sequence of length 1:\n",
    "  # Must be a tuple argument!\n",
    "  target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "  \n",
    "  # Populate the first token of target sequence with the start token:\n",
    "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "  \n",
    "  decoded_sentence = ''\n",
    "\n",
    "  return decoded_sentence\n",
    "\n",
    "for seq_index in range(10):\n",
    "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(test_input)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_docs[seq_index])\n",
    "  print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1aa5b2",
   "metadata": {},
   "source": [
    "### Test Function (part 2)\n",
    "At long last, it’s translation time. Inside the test function, we’ll decode the sentence word by word using the output state that we retrieved from the encoder (which becomes our decoder’s initial hidden state). We’ll also update the decoder hidden state after each word so that we use previously decoded words to help decode new ones.\n",
    "\n",
    "To tackle one word at a time, we need a while loop that will run until one of two things happens (we don’t want the model generating words forever):\n",
    "- The current token is \"\\<END>\".\n",
    "- The decoded Spanish sentence length hits the maximum target sentence length.\n",
    "\n",
    "Inside the while loop, the decoder model can use the current target sequence (beginning with the \"\\<START>\" token) and the current state (initially passed to us from the encoder model) to get a bunch of possible next words and their corresponding probabilities. In Keras, it looks something like this:\n",
    "```\n",
    "output_tokens, new_decoder_hidden_state, new_decoder_cell_state = \n",
    "    decoder_model.predict(\n",
    "    [target_seq] + decoder_states_value)\n",
    "```\n",
    "Next, we can use NumPy’s .argmax() method to determine the token (word) with the highest probability and add it to the decoded sentence:\n",
    "```\n",
    "# slicing [0, -1, :] gives us a\n",
    "# specific token vector within the\n",
    "# 3d NumPy matrix\n",
    "sampled_token_index = np.argmax(\n",
    "    output_tokens[0, -1, :])\n",
    "\n",
    "# The reverse features dictionary\n",
    "# translates back from index to Spanish\n",
    "sampled_token = reverse_target_features_dict[\n",
    "    sampled_token_index]\n",
    "\n",
    "decoded_sentence += \" \" + sampled_token\n",
    "```\n",
    "Our final step is to update a few values for the next word in the sequence:\n",
    "```\n",
    "# Move to the next timestep \n",
    "# of the target sequence:\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "# Update the states with values from\n",
    "# the most recent decoder prediction:\n",
    "decoder_states_value = [\n",
    "    new_decoder_hidden_state,\n",
    "    new_decoder_cell_state]\n",
    "```\n",
    "And now we can test it all out!\n",
    "\n",
    "You may recall that, because of platform constraints here, we’re using very little data. As a result, we can only expect our model to translate a handful of sentences coherently. Luckily, you will have an opportunity to try this out on your own computer with far more data to see some much more impressive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens\n",
    "\n",
    "from prep import target_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, target_tokens\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np\n",
    "\n",
    "training_model = load_model('training_model.h5')\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def decode_sequence(test_input):\n",
    "  encoder_states_value = encoder_model.predict(test_input)\n",
    "  decoder_states_value = encoder_states_value\n",
    "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "  decoded_sentence = ''\n",
    "  \n",
    "  stop_condition = False\n",
    "  while not stop_condition:\n",
    "    # Run the decoder model to get possible \n",
    "    # output tokens (with probabilities) & states\n",
    "    output_tokens, new_decoder_hidden_state, new_decoder_cell_state = decoder_model.predict([target_seq] + decoder_states_value)\n",
    "\n",
    "    # Choose token with highest probability\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    # Use sampled_token_index to find our next word in reverse_target_features_dict. Save the result to sampled_token\n",
    "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "    decoded_sentence += ' ' + sampled_token\n",
    "    # Exit condition: either hit max length\n",
    "    # or find stop token.\n",
    "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "      stop_condition = True\n",
    "\n",
    "    # Update the target sequence (of length 1).\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    # Update states\n",
    "    decoder_states_value = [new_decoder_hidden_state, new_decoder_cell_state]\n",
    "\n",
    "  return decoded_sentence\n",
    "\n",
    "for seq_index in range(10):\n",
    "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(test_input)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_docs[seq_index])\n",
    "  print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cafbec",
   "metadata": {},
   "source": [
    "### Generating Text with Deep Learning Review\n",
    "Congrats! You’ve successfully built a machine translation program using deep learning with Tensorflow’s Keras API.\n",
    "\n",
    "While the translation output may not have been quite what you expected, this is just the beginning. There are many ways you can improve this program on your own device by using a larger data set, increasing the size of the model, and adding more epochs for training.\n",
    "\n",
    "You could also convert the one-hot vectors into word embeddings during training to improve the model. Using embeddings of words rather than one-hot vectors would help the model capture that semantically similar words might have semantically similar embeddings (helping the LSTM generalize).\n",
    "\n",
    "You’ve learned quite a bit, even beyond the Keras implementation:\n",
    "- seq2seq models are deep learning models that use recurrent neural networks like LSTMs to generate output.\n",
    "- In machine translation, seq2seq networks encompass two main parts:\n",
    "    - The encoder accepts language as input and outputs state vectors.\n",
    "    - The decoder accepts the encoder’s final state and outputs possible translations.\n",
    "- Teacher forcing is a method we can use to train seq2seq decoders.\n",
    "- We need to mark the beginning and end of target sentences so that the decoder knows what to expect at the beginning and end of sentences.\n",
    "- One-hot vectors are a way to represent a given word in a set of words wherein we use 1 to indicate the current word and 0 to indicate every other word.\n",
    "- Timesteps help us keep track of where we are in a sentence.\n",
    "- We can adjust batch size, which determines how many sentences we give a model at a time.\n",
    "- We can also tweak dimensionality and number of epochs, which can improve results with careful tuning.\n",
    "- The Softmax function converts the output of the LSTMs into a probability distribution over words in our vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
