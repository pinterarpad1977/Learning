{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1167451a",
   "metadata": {},
   "source": [
    "### Noise Removal\n",
    "Text cleaning is a technique that developers use in a variety of domains. Depending on the goal of your project and where you get your data from, you may want to remove unwanted information, such as:\n",
    "- Punctuation and accents\n",
    "- Special characters\n",
    "- Numeric digits\n",
    "- Leading, ending, and vertical whitespace\n",
    "- HTML formatting\n",
    "\n",
    "The type of noise that you need to remove from text usually depends on its source. For example, you could access data via the Twitter API, scraping a webpage, or voice recognition software. Fortunately, you can use the `.sub()` method in Python’s regular expression (re) library for most of your noise removal needs.\n",
    "\n",
    "The `.sub()` method has three required arguments:\n",
    "- pattern – a regular expression that is searched for in the input string. There must be an r preceding the string to indicate it is a raw string, which treats backslashes as literal characters.\n",
    "- replacement_text – text that replaces all matches in the input string\n",
    "- input – the input string that will be edited by the .sub() method\n",
    "\n",
    "The method returns a string with all instances of the pattern replaced by the replacement_text.\n",
    "Example\n",
    "\n",
    "```First, let’s consider how to remove HTML <p> tags from a string:\n",
    "\n",
    "import re \n",
    "\n",
    "text = \"<p>    This is a paragraph</p>\" \n",
    "result = re.sub(r'<.?p>', '', text)\n",
    "\n",
    "print(result) \n",
    "#    This is a paragraph```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34724e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "\n",
    "#Remove the opening and closing h1 tags\n",
    "headline_no_tag = re.sub(r'</?h1>', '', headline_one)\n",
    "\n",
    "#Remove @ from the tweet\n",
    "tweet_no_at = re.sub(r'@','',tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1815f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenization\n",
    "For many natural language processing tasks, we need access to each word in a string. To access each word, we first have to break the text into smaller components. The method for breaking text into smaller components is called tokenization and the individual components are called tokens.\n",
    "\n",
    "A few common operations that require tokenization include:\n",
    "- Finding how many words or sentences appear in text\n",
    "- Determining how many times a specific word or phrase exists\n",
    "- Accounting for which terms are likely to co-occur\n",
    "- While tokens are usually individual words or terms, they can also be sentences or other size pieces of text.\n",
    "\n",
    "To tokenize individual words, we can use nltk‘s `word_tokenize()` function. The function accepts a string and returns a list of words:\n",
    "```\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenize this text\"\n",
    "tokenized = word_tokenize(text)\n",
    "```\n",
    "\n",
    "To tokenize at the sentence level, we can use `sent_tokenize()` from the same module.\n",
    "```\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Tokenize this sentence. Also, tokenize this sentence.\"\n",
    "tokenized = sent_tokenize(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700d5973",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize\n\u001b[0;32m      3\u001b[0m ecg_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn electrocardiogram is used to record the electrical conduction through a person\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms heart. The readings can be used to diagnose cardiac arrhythmias.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m tokenized_by_word \u001b[38;5;241m=\u001b[39m word_tokenize(ecg_text)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "tokenized_by_word = word_tokenize(ecg_text)\n",
    "tokenized_by_sentence = sent_tokenize(ecg_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626026ec",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Tokenization and noise removal are staples of almost all text pre-processing pipelines. However, some data may require further processing through text normalization. Text normalization is a catch-all term for various text pre-processing tasks. In the next few exercises, we’ll cover a few of them:\n",
    "- Upper or lowercasing\n",
    "- Stopword removal\n",
    "- Stemming – bluntly removing prefixes and suffixes from a word\n",
    "- Lemmatization – replacing a single-word token with its root\n",
    "\n",
    "The simplest of these approaches is to change the case of a string. We can use Python’s built-in String methods to make a string all uppercase or lowercase\n",
    "\n",
    "### Stopword Removal\n",
    "Stopwords are words that we remove during preprocessing when we don’t care about sentence structure. They are usually the most common words in a language and don’t provide any information about the tone of a statement. They include words such as “a”, “an”, and “the”.\n",
    "\n",
    "NLTK provides a built-in library with these words.\n",
    "The process:\n",
    "1. import nltk.corpus stopwords and nltk.tokenize word_tokenize\n",
    "2. initialize stopwords: set(stopwords.words('english'))\n",
    "3. tokenize the text\n",
    "4. with a list comprehension create a list of words without the stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_survey = word_tokenize(survey_text.lower())\n",
    "text_no_stops = [word for word in tokenized_survey if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8acc0",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "In natural language processing, stemming is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes). For example, stemming would cast the word “going” to “go”. This is a common method used by search engines to improve matching between user input and website hits.\n",
    "\n",
    "NLTK has a built-in stemmer called PorterStemmer. You can use it with a list comprehension to stem each word in a tokenized list of words.\n",
    "\n",
    "First, you must import and initialize the stemmer:\n",
    "```\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Now that we have our stemmer, we can apply it to each word in a list using a list comprehension:\n",
    "tokenized = ['NBC', 'was', 'founded', 'in', '1926', '.', 'This', 'makes', 'NBC', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n",
    "\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "print(stemmed)\n",
    "# ['nbc', 'wa', 'found', 'in', '1926', '.', 'thi', 'make', 'nbc', 'the', 'oldest', 'major', 'broadcast', 'network', '.']\n",
    "```\n",
    "Notice, the words like ‘was’ and ‘founded’ became ‘wa’ and ‘found’, respectively. The fact that these words have been reduced is useful for many language processing applications. However, you need to be careful when stemming strings, because words can often be converted to something unrecognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "island_tokenized = word_tokenize(populated_island)\n",
    "stemmed = [stemmer.stem(token) for token in island_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183867f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lemmatization\n",
    "Lemmatization is a method for casting words to their root forms. This is a more involved process than stemming, because it requires the method to know the part of speech for each word. Since lemmatization requires the part of speech, it is a less efficient approach than stemming because lemmatize() treats every word as a noun. To take advantage of the power of lemmatization, we need to tag each word in our text with the most likely part of speech.\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "To improve the performance of lemmatization, we need to find the part of speech for each word in our string. In script.py, to the right, we created a part-of-speech tagging function. The function accepts a word, then returns the most common part of speech for that word. Let’s break down the steps:\n",
    "\n",
    "1. Import wordnet and Counter\n",
    "```\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "```\n",
    "wordnet is a database that we use for contextualizing words\n",
    "Counter is a container that stores elements as dictionary keys\n",
    "\n",
    "2. Get synonyms\n",
    "Inside of our function, we use the wordnet.synsets() function to get a set of synonyms for the word:\n",
    "```\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  ```\n",
    "The returned synonyms come with their part of speech.\n",
    "\n",
    "3. Use synonyms to determine the most likely part of speech\n",
    "Next, we create a Counter() object and set each value to the count of the number of synonyms that fall into each part of speech:\n",
    "\n",
    "`pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )`\n",
    "\n",
    "This line counts the number of nouns in the synonym set.\n",
    "\n",
    "4. Return the most common part of speech\n",
    "Now that we have a count for each part of speech, we can use the .most_common() counter method to find and return the most likely part of speech:\n",
    "\n",
    "most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "\n",
    "Now that we can find the most probable part of speech for a given word, we can pass this into our lemmatizer when we find the root for each word. Because we passed in the part of speech, “is” was cast to its root, “be.” This means that words like “was” and “were” will be cast to “be”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa27ad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indonesia',\n",
       " 'was',\n",
       " 'founded',\n",
       " 'in',\n",
       " '1945',\n",
       " '.',\n",
       " 'It',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'most',\n",
       " 'populated',\n",
       " 'island',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'Java',\n",
       " ',',\n",
       " 'with',\n",
       " 'over',\n",
       " '140',\n",
       " 'million',\n",
       " 'people',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The get part of speech function\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
    "\n",
    "tokenized_string = word_tokenize(populated_island)\n",
    "\n",
    "lemmatized_pos = [ lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized_string]\n",
    "lemmatized_pos[:3]\n",
    "tokenized_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177cb63",
   "metadata": {},
   "source": [
    "1. Book\n",
    "- Processing Raw Text\n",
    "https://www.nltk.org/book/ch03.html\n",
    "\n",
    "2. Videos\n",
    "- Natural Language Processing With Python and NLTK p.1 Tokenizing words and Sentences\n",
    "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n",
    "- Stop Words - Natural Language Processing With Python and NLTK p.2\n",
    "https://pythonprogramming.net/stop-words-nltk-tutorial/\n",
    "- Stemming - Natural Language Processing With Python and NLTK p.3\n",
    "https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "- Lemmatizing - Natural Language Processing With Python and NLTK p.8\n",
    "https://pythonprogramming.net/lemmatizing-nltk-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
