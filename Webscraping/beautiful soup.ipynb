{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46afe651-a577-4a95-88f9-9e38cb3bd33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pip install jupyter-black\n",
    "import jupyter_black\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e8c16-7810-4c6b-bd83-6a008503eda2",
   "metadata": {},
   "source": [
    "### Requests\n",
    "\n",
    "In order to get the HTML of the website, we need to make a request to get the content of the webpage. To learn more about requests in a general sense, you can check out this article.\n",
    "\n",
    "Python has a requests library that makes getting content really easy. All we have to do is import the library, and then feed in the URL we want to GET:\n",
    "```\n",
    "import requests\n",
    "\n",
    "webpage = requests.get('https://www.codecademy.com/articles/http-requests')\n",
    "print(webpage.text)\n",
    "```\n",
    "\n",
    "requests.get(address) creates a response object\n",
    "we'll use mostly the .content or the .text in websraping\n",
    "there is an important difference between them:\n",
    "\n",
    "    Use r.text for textual responses (like HTML or XML).\n",
    "    Use r.content for binary filetypes (such as images or PDFs) or when you need the raw byte stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f7e1a-a58a-44fd-bd4c-7842e2daabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "address = \"https://content.codecademy.com/courses/beautifulsoup/shellter.html\"\n",
    "\n",
    "page_response = requests.get(address)\n",
    "#print(page_response)\n",
    "webpage = page_response.content\n",
    "#print(page_response.text)\n",
    "#print(page_response.content)\n",
    "#print(page_response.headers)\n",
    "#print(page_response.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436ee4f-2ded-4f7f-a670-d81aadf013ac",
   "metadata": {},
   "source": [
    "### The BeautifulSoup Object\n",
    "\n",
    "BeautifulSoup is a Python library that makes it easy for us to traverse an HTML page and pull out the parts we’re interested in. We can import it by using the line:\n",
    "\n",
    "`from bs4 import BeautifulSoup`\n",
    "\n",
    "Then, all we have to do is convert the HTML document to a BeautifulSoup object!\n",
    "\n",
    "`soup = BeautifulSoup(\"rainbow.html\", \"html.parser\")`\n",
    "\n",
    "\"html.parser\" is one option for parsers we could use. \n",
    "There are other options: \"lxml\" , \"html5lib\" \n",
    "\n",
    "```\n",
    "webpage = requests.get(\"http://rainbow.com/rainbow.html\")\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "```\n",
    "When we use BeautifulSoup in combination with pandas, we can turn websites into DataFrames that are easy to manipulate and gain insights from.\n",
    "\n",
    "\n",
    "\n",
    "### Object types in BS\n",
    "---\n",
    "\n",
    "#### Tags\n",
    "\n",
    "(p, div, h, ul, li, a, thead, tbody tfoot, tr, th, td, etc...)\n",
    "We can access the different tags by putting the tag name as a method after the soup object (soup.div)\n",
    "With this call you can access the first such tag on the page.\n",
    "The information within this object can be reaches as in a dictionary:\n",
    "\n",
    "- name of the tag: **soup.div.name**\n",
    "- attributes of the tag: **soup.div.attrs**\n",
    "- the axtual text within the tag: **soup.div.string**\n",
    "\n",
    "\n",
    "#### Navigating by tags\n",
    "\n",
    "you can access the children of a tag: `soup.ul.children`\n",
    "or you can access the parents of a child tag: `soup.li.parents`\n",
    "\n",
    "\n",
    "#### Find\n",
    "\n",
    "- .find() - returns the first tag or none if there is no such tag\n",
    "- -find_all() - returns a list of all the tags or an empty list\n",
    "\n",
    "#### Using Regex\n",
    "\n",
    "What if we want every <ol> and every <ul> that the page contains? We will use the .compile() function from the re module. We will use the regex: \"[ou]l\" which means “match either o or u and l“.\n",
    "\n",
    "We can select both of these types of elements with a regex in our .find_all():\n",
    "```\n",
    "import re\n",
    "soup.find_all(re.compile(\"[ou]l\"))\n",
    "```\n",
    "What if we want all of the h1 - h9 tags that the page contains? Regex to the rescue again! The expression \"h[1-9]\" means h and any number between 1 and 9.\n",
    "```\n",
    "import re\n",
    "soup.find_all(re.compile(\"h[1-9]\"))\n",
    "```\n",
    "\n",
    "we can use lists in find_all instead of just a single attribute:\n",
    "`soup.find_all(['h1', 'a', 'p'])`\n",
    "\n",
    "find_all has an attrs parameter where we can specify dictionaries:\n",
    "`soup.find_all(attrs={'class':'banner'})`\n",
    "`soup.find_all(attrs={'class':'banner', 'id':'jumbotron'})`\n",
    "\n",
    "we can even pass a function into find_all if the selection gets really complicated:\n",
    "```\n",
    "def has_banner_class_and_hello_world(tag):\n",
    "    return tag.attr('class') == \"banner\" and tag.string == \"Hello world\"\n",
    "\n",
    "soup.find_all(has_banner_class_and_hello_world)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1b76e-704a-4b46-aef1-21f9eec2ba6d",
   "metadata": {},
   "source": [
    "#### CSS Selectors .select()\n",
    "\n",
    "Select for CSS Selectors\n",
    "\n",
    "Another way to capture your desired elements with the soup object is to use CSS selectors. The .select() method will take in all of the CSS selectors you normally use in a .css file!\n",
    "```\n",
    "<h1 class='results'>Search Results for: <span class='searchTerm'>Funfetti</span></h1>\n",
    "<div class='recipeLink'><a href=\"spaghetti.html\">Funfetti Spaghetti</a></div>\n",
    "<div class='recipeLink' id=\"selected\"><a href=\"lasagna.html\">Lasagna de Funfetti</a></div>\n",
    "<div class='recipeLink'><a href=\"cupcakes.html\">Funfetti Cupcakes</a></div>\n",
    "<div class='recipeLink'><a href=\"pie.html\">Pecan Funfetti Pie</a></div>\n",
    "```\n",
    "If we wanted to select all of the elements that have the class 'recipeLink', we could use the command:\n",
    "\n",
    "`soup.select(\".recipeLink\")`\n",
    "\n",
    "If we wanted to select the element that has the id 'selected', we could use the command:\n",
    "\n",
    "`soup.select(\"#selected\")`\n",
    "\n",
    "Let’s say we wanted to loop through all of the links to these funfetti recipes that we found from our search.\n",
    "```\n",
    "for link in soup.select(\".recipeLink > a\"):\n",
    "  webpage = requests.get(link)\n",
    "  new_soup = BeautifulSoup(webpage)\n",
    "```\n",
    "This loop will go through each link in each .recipeLink div and create a soup object out of the webpage it links to. So, it would first make soup out of <a href=\"spaghetti.html\">Funfetti Spaghetti</a>, then <a href=\"lasagna.html\">Lasagna de Funfetti</a>, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b564bb0-d14b-4974-961b-bc121b691dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "prefix = \"https://content.codecademy.com/courses/beautifulsoup/\"\n",
    "webpage_response = requests.get(\n",
    "    \"https://content.codecademy.com/courses/beautifulsoup/shellter.html\"\n",
    ")\n",
    "\n",
    "webpage = webpage_response.content\n",
    "soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "\n",
    "turtle_links = soup.find_all(\"a\")\n",
    "links = []\n",
    "# go through all of the a tags and get the links associated with them.\n",
    "# additionally extract the href from the tag and append it to the page prefix to build a link to follow:\n",
    "for a in turtle_links:\n",
    "    links.append(prefix + a[\"href\"])\n",
    "\n",
    "# Define turtle_data to store the data we gather when we follow the links:\n",
    "turtle_data = dict()\n",
    "# follow each link and create a new soup object for each\n",
    "for link in links:\n",
    "    webpage = requests.get(link)\n",
    "    turtle = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    # the select will return a list, we need to take the 0th  index and use the .text to get the actual string\n",
    "    turtle_name = turtle.select(\"p.name\")[0].text\n",
    "    turtle_data[turtle_name] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efe7df-d2cf-48e6-948e-47e30a647696",
   "metadata": {},
   "source": [
    "### Reading text\n",
    "\n",
    "To get the text from a tag we can use the .grt_text() method\n",
    "It will readd all the strings within the tag into a single string.\n",
    "If we want to keep the strings separated we can specify a separator: `.get_text('|')`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5abcf-a532-4442-aae6-225fddd0b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "prefix = \"https://content.codecademy.com/courses/beautifulsoup/\"\n",
    "webpage_response = requests.get(\n",
    "    \"https://content.codecademy.com/courses/beautifulsoup/shellter.html\"\n",
    ")\n",
    "\n",
    "webpage = webpage_response.content\n",
    "soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "\n",
    "turtle_links = soup.find_all(\"a\")\n",
    "links = []\n",
    "# go through all of the a tags and get the links associated with them:\n",
    "for a in turtle_links:\n",
    "    links.append(prefix + a[\"href\"])\n",
    "\n",
    "# Define turtle_data:\n",
    "turtle_data = {}\n",
    "\n",
    "# follow each link and create a new soup for each\n",
    "for link in links:\n",
    "    webpage = requests.get(link)\n",
    "    turtle = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    # parse the name of the turtle based on the class\n",
    "    turtle_name = turtle.select(\".name\")[0].get_text()\n",
    "    # extract the turtle details from the li tags\n",
    "    turtle_details = turtle.find_all(\"li\")\n",
    "    turtle_details_text = []\n",
    "    # as the find_all() return object is a list we need to loop through and extract the text from each element\n",
    "    for detail in turtle_details:\n",
    "        turtle_details_text.append(detail.get_text())\n",
    "    turtle_data[turtle_name] = turtle_details_text\n",
    "print(turtle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842c9b4e-3795-4aa3-9e49-a7d575d90e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0                1            2  \\\n",
      "Aesop        AGE: 7 Years Old    WEIGHT: 6 lbs  SEX: Female   \n",
      "Caesar       AGE: 2 Years Old    WEIGHT: 4 lbs    SEX: Male   \n",
      "Sulla         AGE: 1 Year Old     WEIGHT: 1 lb    SEX: Male   \n",
      "Spyro        AGE: 6 Years Old    WEIGHT: 3 lbs  SEX: Female   \n",
      "Zelda        AGE: 3 Years Old    WEIGHT: 2 lbs  SEX: Female   \n",
      "Bandicoot    AGE: 2 Years Old    WEIGHT: 2 lbs    SEX: Male   \n",
      "Hal           AGE: 1 Year Old  WEIGHT: 1.5 lbs  SEX: Female   \n",
      "Mock        AGE: 10 Years Old   WEIGHT: 10 lbs    SEX: Male   \n",
      "Sparrow    AGE: 1.5 Years Old  WEIGHT: 4.5 lbs  SEX: Female   \n",
      "\n",
      "                                                3  \\\n",
      "Aesop      BREED: African Aquatic Sideneck Turtle   \n",
      "Caesar                      BREED: Greek Tortoise   \n",
      "Sulla      BREED: African Aquatic Sideneck Turtle   \n",
      "Spyro                       BREED: Greek Tortoise   \n",
      "Zelda                   BREED: Eastern Box Turtle   \n",
      "Bandicoot  BREED: African Aquatic Sideneck Turtle   \n",
      "Hal                     BREED: Eastern Box Turtle   \n",
      "Mock                        BREED: Greek Tortoise   \n",
      "Sparrow    BREED: African Aquatic Sideneck Turtle   \n",
      "\n",
      "                                      4  \n",
      "Aesop        SOURCE: found in Lake Erie  \n",
      "Caesar         SOURCE: hatched in house  \n",
      "Sulla        SOURCE: found in Lake Erie  \n",
      "Spyro          SOURCE: hatched in house  \n",
      "Zelda      SOURCE: surrendered by owner  \n",
      "Bandicoot      SOURCE: hatched in house  \n",
      "Hal        SOURCE: surrendered by owner  \n",
      "Mock       SOURCE: surrendered by owner  \n",
      "Sparrow      SOURCE: found in Lake Erie  \n"
     ]
    }
   ],
   "source": [
    "# we can create a dataframe from a dict to store this turtle data\n",
    "# it will need cleaning and transformation though\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "turtle_df = pd.DataFrame.from_dict(turtle_data, orient=\"index\")\n",
    "print(turtle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0174e6-5ed7-4ecf-883c-69adafb45e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656ef4f-b7fb-4338-96b8-c4cfca7e24cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ff24a-f0d9-4ddc-b1a9-cb34b1aaf33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
